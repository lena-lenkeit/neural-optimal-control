{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import evosax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from adaptive import Learner1D, Learner2D\n",
    "from adaptive.notebook_integration import notebook_extension\n",
    "from IPython.display import clear_output\n",
    "from jaxtyping import Array, ArrayLike, PRNGKeyArray, PyTree, Scalar\n",
    "\n",
    "import optimal_control.constraints as constraints\n",
    "import optimal_control.controls as controls\n",
    "import optimal_control.environments.examples as examples\n",
    "import optimal_control.nn as nn\n",
    "import optimal_control.solvers as solvers\n",
    "import optimal_control.trainers as trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_inches = (8.3, 11.7)\n",
    "plot_full_width = a4_inches[0]\n",
    "plot_half_width = a4_inches[0] / 2\n",
    "plot_third_width = a4_inches[0] / 3\n",
    "plot_quarter_width = a4_inches[0] / 4\n",
    "\n",
    "result_base_dir = \"../thesis-results/apoptosis\"\n",
    "plot_style = \"seaborn-paper\"\n",
    "\n",
    "plot_styles = [\"seaborn-v0_8-paper\", \"seaborn-v0_8-talk\"]\n",
    "plot_style_names = [\"\", \"_talk\"]\n",
    "\n",
    "plot_shrink_factor = 0.9\n",
    "\n",
    "plt.style.use(plot_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(save_postfix: str, save_prefix: str = None):\n",
    "    if save_prefix is not None:\n",
    "        plt.savefig(result_base_dir + save_prefix + save_postfix + \".png\", bbox_inches=\"tight\")\n",
    "        plt.savefig(result_base_dir + save_prefix + save_postfix + \".svg\", bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def styles(plot_fn):\n",
    "    for style, name in zip(plot_styles, plot_style_names):\n",
    "        with plt.style.context(style):\n",
    "            plot_fn(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random key for reproducibility\n",
    "key = jax.random.PRNGKey(1234)\n",
    "\n",
    "# Initialize environment\n",
    "environment: examples.ApoptosisEnvironment = examples.ApoptosisEnvironment(\n",
    "    \"../data/Initial_concentrations_CD95H_wtH.mat\", [0, 500], 50\n",
    ")\n",
    "environment_state = environment.init()\n",
    "\n",
    "eval_environment: examples.ApoptosisEnvironment = examples.ApoptosisEnvironment(\n",
    "    \"../data/Initial_concentrations_CD95H_wtH.mat\", [500, 1000], -1, True\n",
    ")\n",
    "eval_environment_state = eval_environment.init()\n",
    "\n",
    "# Build controller\n",
    "key, subkey = jax.random.split(key)\n",
    "control = controls.ImplicitTemporalControl(\n",
    "    implicit_fn=nn.Siren(\n",
    "        in_features=1, out_features=1, hidden_features=64, hidden_layers=2, key=subkey\n",
    "    ),\n",
    "    t_start=0.0,\n",
    "    t_end=180.0,\n",
    "    to_curve=True,\n",
    "    curve_interpolation=\"linear\",\n",
    "    curve_steps=181,\n",
    ")\n",
    "\n",
    "step10_control = controls.ImplicitTemporalControl(\n",
    "    implicit_fn=nn.Siren(\n",
    "        in_features=1, out_features=1, hidden_features=64, hidden_layers=2, key=subkey\n",
    "    ),\n",
    "    t_start=0.0,\n",
    "    t_end=180.0,\n",
    "    to_curve=True,\n",
    "    curve_interpolation=\"linear\",\n",
    "    curve_steps=10,\n",
    ")\n",
    "\n",
    "\n",
    "def tbid_fraction(ys: Array) -> Array:\n",
    "    return ys[..., 12] / (ys[..., 3] + ys[..., 12])\n",
    "\n",
    "\n",
    "# Define reward function\n",
    "def proxy_reward_fn(\n",
    "    args: Tuple[diffrax.Solution, Array],\n",
    "    instantaneous: bool = False,\n",
    "    norm: bool = True,\n",
    "    clip: bool = True,\n",
    "):\n",
    "    # Continuous fraction of tBID, clipped at the tBID-apoptosis threshold\n",
    "\n",
    "    solution, thresh = args\n",
    "    ys = solution.ys\n",
    "\n",
    "    tBID_frac = ys[..., 12] / (ys[..., 3] + ys[..., 12])\n",
    "    if norm:\n",
    "        frac_norm = tBID_frac / thresh\n",
    "        if clip:\n",
    "            frac_clipped = jnp.clip(frac_norm, a_max=1.0)\n",
    "        else:\n",
    "            frac_clipped = frac_norm\n",
    "    else:\n",
    "        if clip:\n",
    "            frac_clipped = jnp.clip(tBID_frac, a_max=thresh)\n",
    "        else:\n",
    "            frac_clipped = tBID_frac\n",
    "\n",
    "    if instantaneous:\n",
    "        reward = jnp.sum(frac_clipped, axis=-1)\n",
    "    else:\n",
    "        reward = jnp.mean(jnp.sum(frac_clipped, axis=-1))\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def true_reward_fn(args: Tuple[diffrax.Solution, Array], instantaneous: bool = False):\n",
    "    # Number of dead cells\n",
    "\n",
    "    solution, thresh = args\n",
    "    ys = solution.ys\n",
    "\n",
    "    tBID_frac = ys[..., 12] / (ys[..., 3] + ys[..., 12])\n",
    "    tBID_above = tBID_frac > thresh\n",
    "\n",
    "    if instantaneous:\n",
    "        reward = jnp.sum(tBID_above, axis=-1)\n",
    "    else:\n",
    "        reward = jnp.mean(jnp.sum(tBID_above, axis=-1))\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "# Direct solver\n",
    "direct_solver = solvers.DirectSolver(optax.adam(learning_rate=3e-4))\n",
    "\n",
    "\n",
    "# ES Solver factory (to allow for init parameters)\n",
    "def make_es_solver(init_control: controls.AbstractControl) -> solvers.ESSolver:\n",
    "    evo_control_params = eqx.filter(init_control, eqx.is_array)\n",
    "    evo_parameter_reshaper = evosax.ParameterReshaper(evo_control_params)\n",
    "    evo_fitness_shaper = evosax.FitnessShaper(centered_rank=True, maximize=True)\n",
    "\n",
    "    # \"\"\"\n",
    "    evo_strategy = evosax.OpenES(\n",
    "        popsize=64,\n",
    "        lrate_init=3e-3,#3e-4,\n",
    "        lrate_limit=3e-3,#3e-4,\n",
    "        sigma_init=1e-3,\n",
    "        sigma_limit=1e-3,\n",
    "        num_dims=len(evo_parameter_reshaper.flatten_single(evo_control_params)),\n",
    "    )\n",
    "    # \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    evo_strategy = evosax.LES(\n",
    "        popsize=64,\n",
    "        num_dims=len(evo_parameter_reshaper.flatten_single(evo_control_params)),\n",
    "        net_ckpt_path=\"../data/2023_03_les_v1.pkl\",\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    evo_strategy_params = evo_strategy.default_params\n",
    "\n",
    "    evo_solver: solvers.ESSolver = solvers.ESSolver(\n",
    "        evo_strategy, evo_strategy_params, evo_parameter_reshaper, evo_fitness_shaper\n",
    "    )\n",
    "\n",
    "    return evo_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helper\n",
    "@eqx.filter_jit\n",
    "def train_with_integral(\n",
    "    control: controls.AbstractControl,\n",
    "    solver: solvers.AbstractSolver,\n",
    "    target_integral: Array,\n",
    "    reward_fn=proxy_reward_fn,\n",
    "    num_steps: int = 1024,\n",
    ") -> Tuple[Scalar, controls.AbstractControl]:\n",
    "    constraint_chain = constraints.ConstraintChain(\n",
    "        transformations=[\n",
    "            constraints.NonNegativeConstantIntegralConstraint(target=target_integral)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    opt_reward, opt_control = trainers.solve_optimal_control_problem(\n",
    "        num_train_steps=num_steps,\n",
    "        environment=environment,\n",
    "        reward_fn=reward_fn,\n",
    "        constraint_chain=constraint_chain,\n",
    "        solver=solver,\n",
    "        control=control,\n",
    "        key=key,\n",
    "        pbar_interval=8,\n",
    "        integrate_kwargs=dict(vmap=\"inner\"),\n",
    "    )\n",
    "\n",
    "    return opt_reward, opt_control\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def evaluate_with_integral(\n",
    "    control: controls.AbstractControl,\n",
    "    target_integral: Array,\n",
    "    environment: examples.ApoptosisEnvironment,\n",
    "    state: examples.ApoptosisState,\n",
    ") -> Tuple[PyTree, controls.AbstractControl]:\n",
    "    constraint_chain = constraints.ConstraintChain(\n",
    "        transformations=[\n",
    "            constraints.NonNegativeConstantIntegralConstraint(target=target_integral)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    constrained_control, _ = solvers.build_control(control, constraint_chain)\n",
    "\n",
    "    solution = environment.integrate(constrained_control, state, key, vmap=\"inner\")\n",
    "    return solution, constrained_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "from typing import Any\n",
    "import dataclasses\n",
    "import importlib\n",
    "\n",
    "\n",
    "class JaxSONEncoder(json.JSONEncoder):\n",
    "    def default(self, o: Any) -> Any:\n",
    "        if dataclasses.is_dataclass(o):\n",
    "            return {\n",
    "                \"__dataclass__\": True,\n",
    "                \"__class_name__\": o.__class__.__module__\n",
    "                + \".\"\n",
    "                + o.__class__.__qualname__,\n",
    "                \"fields\": dataclasses.asdict(o),\n",
    "            }\n",
    "\n",
    "        if isinstance(o, Array):\n",
    "            # Convert to numpy array\n",
    "            o = np.asarray(o)\n",
    "\n",
    "            # Save to memory\n",
    "            buffer = io.BytesIO()\n",
    "            jnp.save(buffer, o, allow_pickle=False)\n",
    "\n",
    "            # Convert the numpy array to a base64 encoded string\n",
    "            data = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "            # Include type annotation and return data\n",
    "            return {\"__numpy_array__\": True, \"data\": data}\n",
    "\n",
    "        # For other types, use the default encoder\n",
    "        return super().default(o)\n",
    "\n",
    "\n",
    "class JaxSONDecoder(json.JSONDecoder):\n",
    "    def __init__(self, dataclass_lookup: dict[str, Any], *args, **kwargs):\n",
    "        super().__init__(object_hook=self.object_hook, *args, **kwargs)\n",
    "\n",
    "        self.dataclass_lookup = dataclass_lookup\n",
    "\n",
    "    def object_hook(self, o: Any) -> Any:\n",
    "        if \"__dataclass__\" in o:\n",
    "            # Construct class instance with __new__\n",
    "            # This allows us to avoid calling __init__, which might be overridden\n",
    "            try:\n",
    "                dataclass_class = self.dataclass_lookup[o[\"__class_name__\"]]\n",
    "            except KeyError:\n",
    "                dataclass_class = importlib.import_module(o[\"__class_name__\"])\n",
    "\n",
    "            dataclass_instance = dataclass_class.__new__(dataclass_class)\n",
    "\n",
    "            dataclass_instance = jax.tree_util.tree_map(\n",
    "                lambda a, b: b, dataclass_instance, o[\"fields\"]\n",
    "            )\n",
    "\n",
    "            # Replace currently undefined fields with saved fields\n",
    "            dataclass_instance = dataclasses.replace(dataclass_instance, **o[\"fields\"])\n",
    "            return dataclass_instance\n",
    "\n",
    "        if \"__numpy_array__\" in o:\n",
    "            # Decode the base64 encoded data\n",
    "            data = base64.b64decode(o[\"data\"])\n",
    "\n",
    "            # Load the numpy array\n",
    "            buffer = io.BytesIO(data)\n",
    "            return jnp.load(buffer)\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between grad, es, and rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_reward, pretrained_control = train_with_integral(\n",
    "    step10_control,\n",
    "    direct_solver,\n",
    "    jnp.asarray([1.0]),\n",
    "    reward_fn=partial(proxy_reward_fn, norm=False),\n",
    "    num_steps=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_reward, finetuned_control = train_with_integral(\n",
    "    pretrained_control,\n",
    "    make_es_solver(pretrained_control),\n",
    "    jnp.asarray([1.0]),\n",
    "    reward_fn=true_reward_fn,\n",
    "    num_steps=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_es_reward, only_es_control = train_with_integral(\n",
    "    control,\n",
    "    make_es_solver(pretrained_control),\n",
    "    jnp.asarray([1.0]),\n",
    "    reward_fn=true_reward_fn,\n",
    "    num_steps=1024 + 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_proxy_reward, es_proxy_control = train_with_integral(\n",
    "    control,\n",
    "    make_es_solver(control),\n",
    "    jnp.asarray([1.0]),\n",
    "    reward_fn=proxy_reward_fn,\n",
    "    num_steps=1024 + 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noclip_reward, noclip_control = train_with_integral(\n",
    "    step10_control,\n",
    "    direct_solver,\n",
    "    jnp.asarray([1.0]),\n",
    "    reward_fn=partial(proxy_reward_fn, norm=False, clip=False),\n",
    "    num_steps=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pretrain_solution, tBID_thresh), pretrain_constrained_control = evaluate_with_integral(\n",
    "    pretrained_control, jnp.asarray([1.0]), eval_environment, eval_environment_state\n",
    ")\n",
    "\n",
    "(\n",
    "    finetuned_solution,\n",
    "    tBID_thresh,\n",
    "), finetune_constrained_control = evaluate_with_integral(\n",
    "    finetuned_control, jnp.asarray([1.0]), eval_environment, eval_environment_state\n",
    ")\n",
    "\n",
    "(\n",
    "    only_es_solution,\n",
    "    tBID_thresh,\n",
    "), only_es_constrained_control = evaluate_with_integral(\n",
    "    only_es_control, jnp.asarray([1.0]), eval_environment, eval_environment_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(noclip_solution, tBID_thresh), noclip_constrained_control = evaluate_with_integral(\n",
    "    noclip_control, jnp.asarray([1.0]), eval_environment, eval_environment_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(es_proxy_solution, tBID_thresh), es_proxy_constrained_control = evaluate_with_integral(\n",
    "    es_proxy_control, jnp.asarray([1.0]), eval_environment, eval_environment_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_proxy_reward = proxy_reward_fn(\n",
    "    (pretrain_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "pretrain_true_reward = true_reward_fn(\n",
    "    (pretrain_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "\n",
    "finetune_proxy_reward = proxy_reward_fn(\n",
    "    (finetuned_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "finetune_true_reward = true_reward_fn(\n",
    "    (finetuned_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "\n",
    "only_es_proxy_reward = proxy_reward_fn(\n",
    "    (only_es_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "only_es_true_reward = true_reward_fn(\n",
    "    (only_es_solution, tBID_thresh), instantaneous=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noclip_proxy_reward = proxy_reward_fn(\n",
    "    (noclip_solution, tBID_thresh), instantaneous=True, clip=False\n",
    ")\n",
    "noclip_true_reward = true_reward_fn((noclip_solution, tBID_thresh), instantaneous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_proxy_reward = proxy_reward_fn(\n",
    "    (es_proxy_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "esp_true_reward = true_reward_fn((es_proxy_solution, tBID_thresh), instantaneous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.mean(pretrain_true_reward), jnp.mean(finetune_true_reward))\n",
    "print(pretrain_true_reward[-1], finetune_true_reward[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqx.tree_serialise_leaves(\n",
    "    result_base_dir + \"/reward_comparison/pretrain_control.eqx\", pretrained_control\n",
    ")\n",
    "eqx.tree_serialise_leaves(\n",
    "    result_base_dir + \"/reward_comparison/finetune_control.eqx\", finetuned_control\n",
    ")\n",
    "eqx.tree_serialise_leaves(\n",
    "    result_base_dir + \"/reward_comparison/es_control.eqx\", only_es_control\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_control = eqx.tree_deserialise_leaves(\n",
    "    result_base_dir + \"/reward_comparison/pretrain_control.eqx\", control\n",
    ")\n",
    "finetuned_control = eqx.tree_deserialise_leaves(\n",
    "    result_base_dir + \"/reward_comparison/finetune_control.eqx\", control\n",
    ")\n",
    "only_es_control = eqx.tree_deserialise_leaves(\n",
    "    result_base_dir + \"/reward_comparison/es_control.eqx\", control\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_es_control.implicit_fn.layers[0].weight, control.implicit_fn.layers[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_proxy_vs_true(save_prefix: str = None):\n",
    "    def plot(style_name):\n",
    "        ts = pretrain_solution.ts\n",
    "\n",
    "        fig, ax = plt.subplots(2, 1, sharex=True, figsize=(plot_half_width, plot_half_width))\n",
    "\n",
    "        ax[0].set_ylabel(\"True Reward\")\n",
    "        ax[0].plot(ts, pretrain_true_reward, label=\"Proxy\")\n",
    "        ax[0].plot(ts, finetune_true_reward, linestyle=\"--\", label=\"Proxy -> True\")\n",
    "        ax[0].plot(ts, only_es_true_reward, label=\"True\")\n",
    "        #ax[0].plot(ts, esp_true_reward, linestyle=\"--\", label=\"ES + Proxy\")\n",
    "        # ax[0].plot(ts, noclip_true_reward, linestyle=\"--\", label=\"Proxy (No-Clip)\")\n",
    "        ax[0].legend()\n",
    "\n",
    "        ax[1].set_xlabel(\"Time [min.]\")\n",
    "        ax[1].set_ylabel(\"Proxy Reward\")\n",
    "        ax[1].plot(ts, pretrain_proxy_reward, label=\"Proxy\")\n",
    "        ax[1].plot(ts, finetune_proxy_reward, linestyle=\"--\", label=\"Proxy -> True\")\n",
    "        ax[1].plot(ts, only_es_proxy_reward, label=\"True\")\n",
    "        #ax[1].plot(ts, esp_proxy_reward, linestyle=\"--\", label=\"ES + Proxy\")\n",
    "        # ax[1].plot(ts, noclip_proxy_reward, linestyle=\"--\", label=\"Proxy (No-Clip)\")\n",
    "\n",
    "        \"\"\"\n",
    "        plt.savefig(\n",
    "            result_base_dir + \"/reward_comparison/proxy_and_true_reward.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.savefig(\n",
    "            result_base_dir + \"/reward_comparison/proxy_and_true_reward.svg\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        show(\"/proxy_vs_true\" + style_name, save_prefix=save_prefix)\n",
    "\n",
    "    styles(plot)\n",
    "\n",
    "plot_proxy_vs_true(save_prefix=\"/reward_comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e3/(180*0.8/(16.6/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_chain = constraints.ConstraintChain(\n",
    "    transformations=[\n",
    "        constraints.NonNegativeConstantIntegralConstraint(target=jnp.asarray([1.0]))\n",
    "    ]\n",
    ")\n",
    "\n",
    "init_constrained_control, _ = solvers.build_control(control, constraint_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pretrain_solution.ts\n",
    "p_cs = jax.vmap(pretrain_constrained_control)(ts)\n",
    "f_cs = jax.vmap(finetune_constrained_control)(ts)\n",
    "es_cs = jax.vmap(only_es_constrained_control)(ts)\n",
    "#esp_cs = jax.vmap(es_proxy_constrained_control)(ts)\n",
    "#nc_cs = jax.vmap(noclip_constrained_control)(ts)\n",
    "#i_cs = jax.vmap(init_constrained_control)(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(style_name):\n",
    "    #plt.figure(figsize=(plot_half_width, plot_half_width))\n",
    "    plt.figure(figsize=(plot_half_width, plot_quarter_width))\n",
    "\n",
    "    plt.xlabel(\"Time [min.]\")\n",
    "    plt.ylabel(\"CD95L [ng/ml]\")\n",
    "    plt.plot(ts, p_cs, label=\"Proxy\")\n",
    "    plt.plot(ts, f_cs, linestyle=\"--\", label=\"Proxy -> True\")\n",
    "    plt.plot(ts, es_cs, label=\"True\")\n",
    "    #plt.plot(ts, esp_cs, label=\"ES + Proxy\")\n",
    "    #plt.plot(ts, i_cs, linestyle=\"--\", label=\"Init\")\n",
    "    #plt.plot(ts, nc_cs, label=\"True\")\n",
    "    plt.legend()\n",
    "\n",
    "    \"\"\"\n",
    "    plt.savefig(\n",
    "        result_base_dir + \"/reward_comparison/proxy_and_true_controls.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.savefig(\n",
    "        result_base_dir + \"/reward_comparison/proxy_and_true_controls.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    #plt.show()\n",
    "    #show(style_name, save_prefix=\"/reward_comparison/proxy_and_true_controls\")\n",
    "    show(style_name, save_prefix=\"/reward_comparison/proxy_and_true_controls_quarter\")\n",
    "\n",
    "styles(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versions for talk\n",
    "\n",
    "def plot_proxy_vs_true(save_prefix: str = None):\n",
    "    def plot(style_name):\n",
    "        ts = pretrain_solution.ts\n",
    "\n",
    "        fig, ax = plt.subplots(2, 1, sharex=True, figsize=(plot_half_width, plot_half_width))\n",
    "\n",
    "        ax[0].set_ylabel(\"Num. Dead Cells\\n(True Reward)\")\n",
    "        ax[0].plot(ts, pretrain_true_reward, label=\"Proxy + Grad.\")\n",
    "        ax[0].plot(ts, only_es_true_reward, label=\"True + ES\")\n",
    "        ax[0].legend()\n",
    "\n",
    "        ax[1].set_xlabel(\"Time [min.]\")\n",
    "        ax[1].set_ylabel(\"Norm. tBID\\n(Proxy Reward)\")\n",
    "        ax[1].plot(ts, pretrain_proxy_reward)\n",
    "        ax[1].plot(ts, only_es_proxy_reward)\n",
    "\n",
    "        show(\"/proxy_vs_true_for_talk\" + style_name, save_prefix=save_prefix)\n",
    "\n",
    "    styles(plot)\n",
    "\n",
    "plot_proxy_vs_true(save_prefix=\"/reward_comparison\")\n",
    "\n",
    "def plot(style_name):\n",
    "    plt.figure(figsize=(plot_half_width, plot_quarter_width))\n",
    "\n",
    "    plt.xlabel(\"Time [min.]\")\n",
    "    plt.ylabel(\"CD95L [ng/ml]\")\n",
    "    plt.plot(ts, p_cs, label=\"Proxy + Grad.\")\n",
    "    plt.plot(ts, es_cs, label=\"True + ES\")\n",
    "    plt.legend()\n",
    "\n",
    "    show(style_name, save_prefix=\"/reward_comparison/proxy_and_true_controls_quarter_for_talk\")\n",
    "\n",
    "styles(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show loss landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate region around optimum\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def sample_direction(network: PyTree, key: PRNGKeyArray) -> PyTree:\n",
    "    jax_params = eqx.filter(network, eqx.is_array)\n",
    "    reshaper = evosax.ParameterReshaper(eqx.filter(network, eqx.is_array))\n",
    "    flat_params = reshaper.flatten_single(jax_params)\n",
    "    noise = jax.random.normal(key, flat_params.shape, flat_params.dtype)\n",
    "    direction = noise / jnp.sqrt(jnp.sum(jnp.square(noise)))\n",
    "\n",
    "    return direction\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def scan_2d(\n",
    "    control: controls.AbstractControl, x: Array, y: Array, xs: Array, ys: Array\n",
    "):\n",
    "    jax_params, jax_static = eqx.partition(control, eqx.is_array)\n",
    "    reshaper = evosax.ParameterReshaper(jax_params)\n",
    "    source_params = reshaper.flatten_single(jax_params)\n",
    "    offsets = y[None, None] * ys[:, None, None] + x[None, None] * xs[None, :, None]\n",
    "    offsets = offsets.reshape(-1, len(source_params))\n",
    "    grid_params = source_params + offsets\n",
    "\n",
    "    # \"\"\"\n",
    "    grid_controls = eqx.combine(reshaper.reshape(grid_params), jax_static)\n",
    "\n",
    "    (\n",
    "        grid_solutions,\n",
    "        grid_tbid_thresholds,\n",
    "    ), _ = eqx.filter_vmap(\n",
    "        partial(\n",
    "            evaluate_with_integral,\n",
    "            target_integral=jnp.asarray([1.0]),\n",
    "            environment=eval_environment,\n",
    "            state=eval_environment_state,\n",
    "        )\n",
    "    )(grid_controls)\n",
    "    # \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    grid_controls = eqx.combine(reshaper.reshape(grid_params), jax_static)\n",
    "    grid_solutions, grid_tBID_thresholds = jax.lax.map(\n",
    "        partial(\n",
    "            evaluate_with_integral,\n",
    "            target_integral=jnp.asarray([1.0]),\n",
    "            environment=eval_environment,\n",
    "            state=eval_environment_state,\n",
    "        ),\n",
    "        grid_controls,\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    grid_solutions = []\n",
    "    grid_tbid_thresholds = []\n",
    "    for params in grid_params:\n",
    "        grid_control = eqx.combine(reshaper.reshape_single(params), jax_static)\n",
    "        grid_solution, grid_tbid_threshold = evaluate_with_integral(\n",
    "            control=grid_control,\n",
    "            target_integral=jnp.asarray([1.0]),\n",
    "            environment=eval_environment,\n",
    "            state=eval_environment_state,\n",
    "        )\n",
    "\n",
    "        grid_solutions.append(grid_solutions)\n",
    "        grid_tbid_thresholds.append(grid_tbid_thresholds)\n",
    "\n",
    "    grid_solutions = jax.tree_map(\n",
    "        lambda x: jnp.stack(x, axis=0) if eqx.is_array(x[0]) else x[0],\n",
    "        grid_solutions[0],\n",
    "        grid_solutions[1:],\n",
    "    )\n",
    "    grid_tbid_thresholds = jnp.stack(grid_tbid_thresholds, axis=0)\n",
    "    \"\"\"\n",
    "\n",
    "    return grid_solutions, grid_tbid_thresholds\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def sample_control(\n",
    "    control: controls.AbstractControl, x_dir: Array, y_dir: Array, x: Scalar, y: Scalar\n",
    ") -> controls.AbstractControl:\n",
    "    jax_params, jax_static = eqx.partition(control, eqx.is_array)\n",
    "    reshaper = evosax.ParameterReshaper(jax_params)\n",
    "    source_params = reshaper.flatten_single(jax_params)\n",
    "\n",
    "    offset = y_dir * y + x_dir * x\n",
    "    offset_params = source_params + offset\n",
    "    offset_params = reshaper.reshape_single(offset_params)\n",
    "    offset_control = eqx.combine(offset_params, jax_static)\n",
    "\n",
    "    return offset_control\n",
    "\n",
    "\n",
    "def adaptive_2d(\n",
    "    control: controls.AbstractControl,\n",
    "    x_dir: Array,\n",
    "    y_dir: Array,\n",
    "    bounds: Tuple[float],\n",
    "    reward_fn,\n",
    "    max_points: int = 1204,\n",
    "):\n",
    "    learner = Learner2D(lambda _: 0.0, bounds=bounds)\n",
    "    learner.stack_size = 1\n",
    "\n",
    "    last_plt_time = time.time()\n",
    "    plt_interval = 15\n",
    "\n",
    "    while learner.npoints < max_points:\n",
    "        try:\n",
    "            points, _ = learner.ask(1)\n",
    "            point = points[0]\n",
    "            x, y = point\n",
    "\n",
    "            offset_control = sample_control(\n",
    "                control, x_dir, y_dir, jnp.float_(x), jnp.float_(y)\n",
    "            )\n",
    "            offset_solution, _ = evaluate_with_integral(\n",
    "                offset_control,\n",
    "                jnp.asarray([1.0]),\n",
    "                eval_environment,\n",
    "                eval_environment_state,\n",
    "            )\n",
    "            offset_reward = reward_fn(offset_solution)\n",
    "            learner.tell(point, float(offset_reward))\n",
    "\n",
    "            if time.time() - last_plt_time >= plt_interval:\n",
    "                clear_output(wait=True)\n",
    "                display(learner.plot(tri_alpha=0.25))\n",
    "                last_plt_time = time.time()\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xkey, ykey = jax.random.split(key)\n",
    "\n",
    "x = sample_direction(pretrained_control, xkey)\n",
    "y = sample_direction(pretrained_control, ykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This consumes lots of RAM\n",
    "\n",
    "grid_solutions, grid_tBID_thresholds = scan_2d(\n",
    "    pretrained_control, x, y, jnp.linspace(-1.5, 1.5, 16), jnp.linspace(-1.5, 1.5, 16)\n",
    ")\n",
    "\n",
    "es_grid_solutions, es_grid_tBID_thresholds = scan_2d(\n",
    "    only_es_control, x, y, jnp.linspace(-1.5, 1.5, 16), jnp.linspace(-1.5, 1.5, 16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_proxy_learner = adaptive_2d(\n",
    "    pretrained_control,\n",
    "    x,\n",
    "    y,\n",
    "    [(-1.5, 1.5), (-1.5, 1.5)],\n",
    "    reward_fn=proxy_reward_fn,\n",
    "    max_points=1024,\n",
    ")\n",
    "\n",
    "pretrain_true_learner = adaptive_2d(\n",
    "    pretrained_control,\n",
    "    x,\n",
    "    y,\n",
    "    [(-1.5, 1.5), (-1.5, 1.5)],\n",
    "    reward_fn=true_reward_fn,\n",
    "    max_points=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_proxy_learner = adaptive_2d(\n",
    "    only_es_control,\n",
    "    x,\n",
    "    y,\n",
    "    [(-1.5, 1.5), (-1.5, 1.5)],\n",
    "    reward_fn=proxy_reward_fn,\n",
    "    max_points=1024,\n",
    ")\n",
    "\n",
    "es_true_learner = adaptive_2d(\n",
    "    only_es_control,\n",
    "    x,\n",
    "    y,\n",
    "    [(-1.5, 1.5), (-1.5, 1.5)],\n",
    "    reward_fn=true_reward_fn,\n",
    "    max_points=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_proxy_learner.save(\n",
    "    result_base_dir + \"/reward_comparison/pretrain_proxy_learner.pickle\"\n",
    ")\n",
    "pretrain_true_learner.save(\n",
    "    result_base_dir + \"/reward_comparison/pretrain_true_learner.pickle\"\n",
    ")\n",
    "es_proxy_learner.save(result_base_dir + \"/reward_comparison/es_proxy_learner.pickle\")\n",
    "es_true_learner.save(result_base_dir + \"/reward_comparison/es_true_learner.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_proxy_learner = Learner2D(lambda x: 0, [(-1.5, 1.5), (-1.5, 1.5)])\n",
    "pretrain_proxy_learner.load(\n",
    "    result_base_dir + \"/reward_comparison/pretrain_proxy_learner.pickle\"\n",
    ")\n",
    "pretrain_true_learner = Learner2D(lambda x: 0, [(-1.5, 1.5), (-1.5, 1.5)])\n",
    "pretrain_true_learner.load(\n",
    "    result_base_dir + \"/reward_comparison/pretrain_true_learner.pickle\"\n",
    ")\n",
    "es_proxy_learner = Learner2D(lambda x: 0, [(-1.5, 1.5), (-1.5, 1.5)])\n",
    "es_proxy_learner.load(result_base_dir + \"/reward_comparison/es_proxy_learner.pickle\")\n",
    "es_true_learner = Learner2D(lambda x: 0, [(-1.5, 1.5), (-1.5, 1.5)])\n",
    "es_true_learner.load(result_base_dir + \"/reward_comparison/es_true_learner.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot from vmapped grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_proxy_rewards = eqx.filter_vmap(proxy_reward_fn)(\n",
    "    (grid_solutions, grid_tBID_thresholds)\n",
    ")\n",
    "grid_proxy_no_norm_rewards = eqx.filter_vmap(partial(proxy_reward_fn, norm=False))(\n",
    "    (grid_solutions, grid_tBID_thresholds)\n",
    ")\n",
    "grid_true_rewards = eqx.filter_vmap(true_reward_fn)(\n",
    "    (grid_solutions, grid_tBID_thresholds)\n",
    ")\n",
    "\n",
    "es_grid_proxy_rewards = eqx.filter_vmap(proxy_reward_fn)(\n",
    "    (grid_solutions, grid_tBID_thresholds)\n",
    ")\n",
    "es_grid_proxy_no_norm_rewards = eqx.filter_vmap(partial(proxy_reward_fn, norm=False))(\n",
    "    (grid_solutions, grid_tBID_thresholds)\n",
    ")\n",
    "es_grid_true_rewards = eqx.filter_vmap(true_reward_fn)(\n",
    "    (grid_solutions, grid_tBID_thresholds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter([0.0], [0.0], c=\"red\", marker=\"x\")\n",
    "plt.imshow(\n",
    "    grid_proxy_rewards.reshape(16, 16), cmap=\"magma\", extent=(-1.5, 1.5, -1.5, 1.5)\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter([0.0], [0.0], c=\"red\", marker=\"x\")\n",
    "plt.imshow(\n",
    "    grid_proxy_no_norm_rewards.reshape(16, 16),\n",
    "    cmap=\"magma\",\n",
    "    extent=(-1.5, 1.5, -1.5, 1.5),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter([0.0], [0.0], c=\"red\", marker=\"x\")\n",
    "plt.imshow(\n",
    "    grid_true_rewards.reshape(16, 16), cmap=\"magma\", extent=(-1.5, 1.5, -1.5, 1.5)\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot from adaptive grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_proxy_reward_grid = pretrain_proxy_learner.interpolated_on_grid()\n",
    "pretrain_true_reward_grid = pretrain_true_learner.interpolated_on_grid()\n",
    "es_proxy_reward_grid = es_proxy_learner.interpolated_on_grid()\n",
    "es_true_reward_grid = es_true_learner.interpolated_on_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from resize_right import resize, interp_methods\n",
    "\n",
    "\n",
    "def train_regressor(grid_in, grid_out):\n",
    "    grid_in = resize(\n",
    "        grid_in,\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "        pad_mode=\"edge\",\n",
    "    )\n",
    "    grid_out = resize(\n",
    "        grid_out,\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "        pad_mode=\"edge\",\n",
    "    )\n",
    "\n",
    "    regressor = LinearRegression(n_jobs=-1).fit(\n",
    "        grid_in.flatten()[:, None],\n",
    "        grid_out.flatten()[:, None],\n",
    "    )\n",
    "    grid_pred = regressor.predict(grid_in.flatten()[:, None])\n",
    "\n",
    "    r2 = regressor.score(grid_in.flatten()[:, None], grid_out.flatten()[:, None])\n",
    "\n",
    "    return regressor, grid_in, grid_out, grid_pred, r2\n",
    "\n",
    "\n",
    "pretrain_regression = train_regressor(\n",
    "    pretrain_proxy_reward_grid[2], pretrain_true_reward_grid[2]\n",
    ")\n",
    "es_regression = train_regressor(es_proxy_reward_grid[2], es_true_reward_grid[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resize_right import resize, interp_methods\n",
    "\n",
    "\n",
    "def plot_reward_grid(grid_x, grid_y, grid_value, label: str, filepath: str):\n",
    "    def plot(style_name):\n",
    "        plt.figure(\n",
    "            figsize=(\n",
    "                plot_half_width * plot_shrink_factor,\n",
    "                plot_half_width * plot_shrink_factor,\n",
    "            )\n",
    "        )\n",
    "        plt.xlabel(\"X Offset [a.u.]\")\n",
    "        plt.ylabel(\"Y Offset [a.u.]\")\n",
    "        plt.imshow(\n",
    "            grid_value,\n",
    "            cmap=\"magma\",\n",
    "            extent=(grid_x[0], grid_x[-1], grid_y[0], grid_y[-1]),\n",
    "        )\n",
    "        cbar = plt.colorbar(fraction=0.04575, pad=0.04)\n",
    "        cbar.set_label(label)\n",
    "        # plt.savefig(result_base_dir + filepath + \".png\", bbox_inches=\"tight\")\n",
    "        # plt.savefig(result_base_dir + filepath + \".svg\", bbox_inches=\"tight\")\n",
    "        # plt.show()\n",
    "\n",
    "        show(style_name, save_prefix=filepath)\n",
    "\n",
    "    styles(plot)\n",
    "\n",
    "\n",
    "def plot_diff_grid(\n",
    "    grid_x, grid_y, grid1_value, grid2_value, filepath: str, norm: bool = True\n",
    "):\n",
    "    grid1_value = resize(\n",
    "        grid1_value,\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "        pad_mode=\"edge\",\n",
    "    )\n",
    "    grid2_value = resize(\n",
    "        grid2_value,\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "        pad_mode=\"edge\",\n",
    "    )\n",
    "\n",
    "    if norm:\n",
    "        grid1_value = (grid1_value - grid1_value.min()) / (\n",
    "            grid1_value.max() - grid1_value.min()\n",
    "        )\n",
    "        grid2_value = (grid2_value - grid2_value.min()) / (\n",
    "            grid2_value.max() - grid2_value.min()\n",
    "        )\n",
    "\n",
    "    # While znorm leads to easy-to-interpret differences, the data isn't normally\n",
    "    # distributed\n",
    "    \"\"\"\n",
    "    grid1_value = (grid1_value - grid1_value.mean()) / grid1_value.std()\n",
    "    grid2_value = (grid2_value - grid2_value.mean()) / grid2_value.std()\n",
    "    \"\"\"\n",
    "\n",
    "    # grid1_value = grid1_value / grid1_value.mean()\n",
    "    # grid2_value = grid2_value / grid2_value.mean()\n",
    "\n",
    "    grid_diff = grid1_value - grid2_value\n",
    "    # grid_diff = (grid2_value / grid1_value) * 100 - 100\n",
    "    vabs = np.abs(grid_diff).max()\n",
    "\n",
    "    def plot(style_name):\n",
    "        plt.figure(\n",
    "            figsize=(\n",
    "                plot_half_width * plot_shrink_factor,\n",
    "                plot_half_width * plot_shrink_factor,\n",
    "            )\n",
    "        )\n",
    "        plt.xlabel(\"X Offset [a.u.]\")\n",
    "        plt.ylabel(\"Y Offset [a.u.]\")\n",
    "        plt.imshow(\n",
    "            grid_diff,\n",
    "            cmap=\"RdBu\",\n",
    "            extent=(grid_x[0], grid_x[-1], grid_y[0], grid_y[-1]),\n",
    "            vmin=-vabs,\n",
    "            vmax=vabs,\n",
    "        )\n",
    "        cbar = plt.colorbar(fraction=0.04575, pad=0.04)\n",
    "        cbar.set_label(\n",
    "            \"Diff. between norm. rewards\" if norm else \"Diff. between rewards\"\n",
    "        )\n",
    "        # plt.savefig(result_base_dir + filepath + \".png\", bbox_inches=\"tight\")\n",
    "        # plt.savefig(result_base_dir + filepath + \".svg\", bbox_inches=\"tight\")\n",
    "        # plt.show()\n",
    "\n",
    "        show(style_name, save_prefix=filepath)\n",
    "\n",
    "    styles(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x, grid_y = pretrain_proxy_reward_grid[:2]\n",
    "\n",
    "plot_reward_grid(\n",
    "    *pretrain_proxy_reward_grid,\n",
    "    label=\"Proxy Reward\",\n",
    "    filepath=\"/reward_comparison/pretrain_proxy_reward_grid\"\n",
    ")\n",
    "plot_reward_grid(\n",
    "    *pretrain_true_reward_grid,\n",
    "    label=\"True Reward\",\n",
    "    filepath=\"/reward_comparison/pretrain_true_reward_grid\"\n",
    ")\n",
    "\n",
    "plot_reward_grid(\n",
    "    *es_proxy_reward_grid,\n",
    "    label=\"Proxy Reward\",\n",
    "    filepath=\"/reward_comparison/es_proxy_reward_grid\"\n",
    ")\n",
    "plot_reward_grid(\n",
    "    *es_true_reward_grid,\n",
    "    label=\"True Reward\",\n",
    "    filepath=\"/reward_comparison/es_true_reward_grid\"\n",
    ")\n",
    "\n",
    "plot_diff_grid(\n",
    "    grid_x,\n",
    "    grid_y,\n",
    "    pretrain_proxy_reward_grid[2],\n",
    "    pretrain_true_reward_grid[2],\n",
    "    filepath=\"/reward_comparison/pretrain_diff_reward_grid\",\n",
    ")\n",
    "\n",
    "plot_diff_grid(\n",
    "    grid_x,\n",
    "    grid_y,\n",
    "    es_proxy_reward_grid[2],\n",
    "    es_true_reward_grid[2],\n",
    "    filepath=\"/reward_comparison/es_diff_reward_grid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x, grid_y = pretrain_proxy_reward_grid[:2]\n",
    "\n",
    "plot_diff_grid(\n",
    "    grid_x,\n",
    "    grid_y,\n",
    "    pretrain_regression[2].reshape(64, 64),\n",
    "    pretrain_regression[3].reshape(64, 64),\n",
    "    filepath=\"/reward_comparison/pretrain_diff_reward_grid_regression\",\n",
    "    norm=False,\n",
    ")\n",
    "\n",
    "plot_diff_grid(\n",
    "    grid_x,\n",
    "    grid_y,\n",
    "    es_regression[2].reshape(64, 64),\n",
    "    es_regression[3].reshape(64, 64),\n",
    "    filepath=\"/reward_comparison/es_diff_reward_grid_regression\",\n",
    "    norm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_regression[-1], es_regression[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(\n",
    "    resize(\n",
    "        pretrain_proxy_reward_grid[2],\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "    ).flatten(),\n",
    "    bins=128,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    resize(\n",
    "        pretrain_proxy_reward_grid[2],\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "    ).flatten(),\n",
    "    resize(\n",
    "        pretrain_true_reward_grid[2],\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "    ).flatten(),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling over CD95L integrals & comparison to constant CD95L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adaptive_1d(\n",
    "    base_control: controls.AbstractControl,\n",
    "    bounds: Tuple[float],\n",
    "    max_points: int = 1204,\n",
    "):\n",
    "    learner = Learner1D(lambda _: 0.0, bounds=bounds)\n",
    "    learner.stack_size = 1\n",
    "\n",
    "    last_plt_time = time.time()\n",
    "    plt_interval = 15\n",
    "\n",
    "    extra_infos = []\n",
    "    while learner.npoints < max_points:\n",
    "        try:\n",
    "            points, _ = learner.ask(1)\n",
    "            point = points[0]\n",
    "            log_mean_conc = point\n",
    "            mean_conc = jnp.asarray([10**log_mean_conc])\n",
    "\n",
    "            optimized_reward, optimized_control = train_with_integral(\n",
    "                base_control, direct_solver, mean_conc, num_steps=1024\n",
    "            )\n",
    "\n",
    "            (eval_solution, eval_threshold), eval_control = evaluate_with_integral(\n",
    "                optimized_control,\n",
    "                mean_conc,\n",
    "                eval_environment,\n",
    "                eval_environment_state,\n",
    "            )\n",
    "\n",
    "            eval_proxy_reward = proxy_reward_fn((eval_solution, eval_threshold))\n",
    "            eval_true_reward = true_reward_fn((eval_solution, eval_threshold))\n",
    "\n",
    "            extra_infos.append(\n",
    "                {\n",
    "                    \"log_mean_conc\": log_mean_conc,\n",
    "                    \"mean_conc\": mean_conc,\n",
    "                    \"optimized_reward\": optimized_reward,\n",
    "                    \"optimized_control\": optimized_control,\n",
    "                    \"eval_solution\": eval_solution,\n",
    "                    \"eval_threshold\": eval_threshold,\n",
    "                    \"eval_control\": eval_control,\n",
    "                    \"eval_proxy_reward\": eval_proxy_reward,\n",
    "                    \"eval_true_reward\": eval_true_reward,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            learner.tell(point, float(eval_proxy_reward))\n",
    "\n",
    "            if time.time() - last_plt_time >= plt_interval:\n",
    "                clear_output(wait=True)\n",
    "                display(learner.plot())\n",
    "                last_plt_time = time.time()\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    return learner, extra_infos\n",
    "\n",
    "\n",
    "def constant_adaptive_1d(\n",
    "    bounds: Tuple[float],\n",
    "    max_points: int = 1204,\n",
    "):\n",
    "    learner = Learner1D(lambda _: 0.0, bounds=bounds)\n",
    "    learner.stack_size = 1\n",
    "\n",
    "    last_plt_time = time.time()\n",
    "    plt_interval = 15\n",
    "\n",
    "    extra_infos = []\n",
    "    while learner.npoints < max_points:\n",
    "        try:\n",
    "            points, _ = learner.ask(1)\n",
    "            point = points[0]\n",
    "            log_mean_conc = point\n",
    "            mean_conc = jnp.asarray([10**log_mean_conc])\n",
    "\n",
    "            constant_control = controls.InterpolationCurveControl(\n",
    "                nn.InterpolationCurve(\n",
    "                    method=\"step\", t_start=0.0, t_end=180.0, steps=1, channels=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "            (eval_solution, eval_threshold), eval_control = evaluate_with_integral(\n",
    "                constant_control,\n",
    "                mean_conc,\n",
    "                eval_environment,\n",
    "                eval_environment_state,\n",
    "            )\n",
    "\n",
    "            eval_proxy_reward = proxy_reward_fn((eval_solution, eval_threshold))\n",
    "            eval_true_reward = true_reward_fn((eval_solution, eval_threshold))\n",
    "\n",
    "            extra_infos.append(\n",
    "                {\n",
    "                    \"log_mean_conc\": log_mean_conc,\n",
    "                    \"mean_conc\": mean_conc,\n",
    "                    \"eval_solution\": eval_solution,\n",
    "                    \"eval_threshold\": eval_threshold,\n",
    "                    \"eval_proxy_reward\": eval_proxy_reward,\n",
    "                    \"eval_true_reward\": eval_true_reward,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            learner.tell(point, float(eval_proxy_reward))\n",
    "\n",
    "            if time.time() - last_plt_time >= plt_interval:\n",
    "                clear_output(wait=True)\n",
    "                display(learner.plot())\n",
    "                last_plt_time = time.time()\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    return learner, extra_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_control = controls.InterpolationCurveControl(\n",
    "    nn.InterpolationCurve(method=\"step\", t_start=0.0, t_end=180.0, steps=10, channels=1)\n",
    ")\n",
    "\n",
    "cont_train_learner, cont_train_extra_infos = train_adaptive_1d(\n",
    "    control, (-5, 5), max_points=80\n",
    ")\n",
    "step_train_learner, step_train_extra_infos = train_adaptive_1d(\n",
    "    step_control, (-5, 5), max_points=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_learner, constant_extra_infos = constant_adaptive_1d((-5, 5), max_points=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_learner.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(learner, extra_infos, prefix):\n",
    "    learner.save(result_base_dir + prefix + \"_learner.pickle\")\n",
    "\n",
    "    eqx.tree_serialise_leaves(\n",
    "        result_base_dir + prefix + \"_extra_infos.eqx\", extra_infos\n",
    "    )\n",
    "\n",
    "    with open(result_base_dir + prefix + \"_extra_infos.json\", mode=\"w\") as f:\n",
    "        json.dump(extra_infos, f, indent=4, cls=JaxSONEncoder)\n",
    "\n",
    "\n",
    "save(cont_train_learner, cont_train_extra_infos, \"/scans/continuous/cont_train\")\n",
    "save(step_train_learner, step_train_extra_infos, \"/scans/step/step_train\")\n",
    "save(constant_learner, constant_extra_infos, \"/scans/constant/constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_train_learner = Learner1D(lambda x: 0, (-5, 5))\n",
    "cont_train_learner.load(\"/scans/continuous/cont_train_learner.pickle\")\n",
    "\n",
    "constant_learner = Learner1D(lambda x: 0, (-5, 5))\n",
    "constant_learner.load(\"/scans/constant/constant_learner.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(prefix, constant: bool):\n",
    "    learner = Learner1D(lambda x: 0, (-5, 5))\n",
    "    learner.load(result_base_dir + prefix + \"_learner.pickle\")\n",
    "\n",
    "    log_mean_conc = 0.0\n",
    "    mean_conc = jnp.zeros(1)\n",
    "    optimized_reward = jnp.float_(0.0)\n",
    "    optimized_control = control\n",
    "    (eval_solution, eval_threshold), eval_control = evaluate_with_integral(\n",
    "        control, jnp.asarray([1.0]), eval_environment, eval_environment_state\n",
    "    )\n",
    "\n",
    "    eval_proxy_reward = proxy_reward_fn((eval_solution, eval_threshold))\n",
    "    eval_true_reward = true_reward_fn((eval_solution, eval_threshold))\n",
    "\n",
    "    extra_infos = [\n",
    "        {\n",
    "            \"log_mean_conc\": log_mean_conc,\n",
    "            \"mean_conc\": mean_conc,\n",
    "            \"eval_solution\": eval_solution,\n",
    "            \"eval_threshold\": eval_threshold,\n",
    "            \"eval_proxy_reward\": eval_proxy_reward,\n",
    "            \"eval_true_reward\": eval_true_reward,\n",
    "        }\n",
    "        if constant\n",
    "        else {\n",
    "            \"log_mean_conc\": log_mean_conc,\n",
    "            \"mean_conc\": mean_conc,\n",
    "            \"optimized_reward\": optimized_reward,\n",
    "            \"optimized_control\": optimized_control,\n",
    "            \"eval_solution\": eval_solution,\n",
    "            \"eval_threshold\": eval_threshold,\n",
    "            \"eval_control\": eval_control,\n",
    "            \"eval_proxy_reward\": eval_proxy_reward,\n",
    "            \"eval_true_reward\": eval_true_reward,\n",
    "        }\n",
    "    ] * learner.npoints\n",
    "\n",
    "    extra_infos = eqx.tree_deserialise_leaves(\n",
    "        result_base_dir + prefix + \"_extra_infos.eqx\", extra_infos\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    with open(result_base_dir + prefix + \"_extra_infos.json\", mode=\"r\") as f:\n",
    "        extra_infos = json.load(\n",
    "            f,\n",
    "            dataclass_lookup={\n",
    "                \"optimal_control.controls.controls.ImplicitTemporalControl\": controls.ImplicitTemporalControl\n",
    "            },\n",
    "            cls=JaxSONDecoder,\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    return learner, extra_infos\n",
    "\n",
    "\n",
    "cont_train_learner, cont_train_extra_infos = load(\"/scans/continuous/cont_train\", False)\n",
    "constant_learner, constant_extra_infos = load(\"/scans/constant/constant\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_volume = 0.8  # ml\n",
    "concentration_to_moles = 16.6 / 500  # 16.6nM = 500ng/ml\n",
    "conversion_factor = reaction_volume / concentration_to_moles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e-5 * conversion_factor * 180, 1e5 * conversion_factor * 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.07e3 / 6.19e2, 5.71e3 / 1.27e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{3000 * 180.0:.2e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "def find_closest_entry(lst, key, target):\n",
    "    closest_index = None\n",
    "    closest_diff = float(\"inf\")\n",
    "\n",
    "    for i, entry in enumerate(lst):\n",
    "        if key in entry:\n",
    "            diff = abs(entry[key] - target)\n",
    "\n",
    "            if diff < closest_diff:\n",
    "                closest_diff = diff\n",
    "                closest_index = i\n",
    "\n",
    "    return closest_index\n",
    "\n",
    "\n",
    "# ChatGPT\n",
    "def linear_interp(x_values, y_values, target_y):\n",
    "    # Find the indices of the two points surrounding the target y value\n",
    "    idx = np.searchsorted(y_values, target_y)\n",
    "\n",
    "    # Ensure that the index is within the bounds of the array\n",
    "    if idx >= len(y_values):\n",
    "        idx = len(y_values) - 1\n",
    "\n",
    "    # Calculate the slope between the two surrounding points\n",
    "    slope = (x_values[idx] - x_values[idx - 1]) / (y_values[idx] - y_values[idx - 1])\n",
    "\n",
    "    # Calculate the interpolated x value\n",
    "    interp_x = x_values[idx - 1] + slope * (target_y - y_values[idx - 1])\n",
    "\n",
    "    return interp_x\n",
    "\n",
    "\n",
    "def sort_by_key(\n",
    "    x: List[dict], sort_key: str, value_keys: str, key_map, value_map\n",
    ") -> Tuple[list, list]:\n",
    "    values = [\n",
    "        (\n",
    "            key_map(v[sort_key]),\n",
    "            value_map(*[v[k] for k in value_keys])\n",
    "            if isinstance(value_keys, (tuple, list))\n",
    "            else value_map(v[value_keys]),\n",
    "        )\n",
    "        for v in x\n",
    "    ]\n",
    "    return sorted(values, key=lambda v: v[0])\n",
    "\n",
    "\n",
    "def final_dead_cells(solution, thresholds):\n",
    "    dead_cells = true_reward_fn((solution, thresholds), instantaneous=True)\n",
    "    return dead_cells[-1]\n",
    "\n",
    "\n",
    "train_proxy = np.asarray(\n",
    "    sort_by_key(\n",
    "        cont_train_extra_infos,\n",
    "        \"log_mean_conc\",\n",
    "        \"eval_proxy_reward\",\n",
    "        key_map=lambda k: k,\n",
    "        value_map=lambda v: v,\n",
    "    )\n",
    ")\n",
    "constant_proxy = np.asarray(\n",
    "    sort_by_key(\n",
    "        constant_extra_infos,\n",
    "        \"log_mean_conc\",\n",
    "        \"eval_proxy_reward\",\n",
    "        key_map=lambda k: k,\n",
    "        value_map=lambda v: v,\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_dead = np.asarray(\n",
    "    sort_by_key(\n",
    "        cont_train_extra_infos,\n",
    "        \"log_mean_conc\",\n",
    "        (\"eval_solution\", \"eval_threshold\"),\n",
    "        key_map=lambda k: k,\n",
    "        value_map=final_dead_cells,\n",
    "    )\n",
    ")\n",
    "constant_dead = np.asarray(\n",
    "    sort_by_key(\n",
    "        constant_extra_infos,\n",
    "        \"log_mean_conc\",\n",
    "        (\"eval_solution\", \"eval_threshold\"),\n",
    "        key_map=lambda k: k,\n",
    "        value_map=final_dead_cells,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def tot_cd95l_at_percentile(x, y, p):\n",
    "    x = linear_interp(x, y, y.max() * p)\n",
    "    return 10**x * conversion_factor * 180.0\n",
    "\n",
    "\n",
    "for p in [0.05, 0.5, 0.95]:\n",
    "    print(\n",
    "        f\"train_dead {p} {tot_cd95l_at_percentile(train_dead[..., 0], train_dead[..., 1], p):.2e}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"constant_dead {p} {tot_cd95l_at_percentile(constant_dead[..., 0], constant_dead[..., 1], p):.2e}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"train_proxy {p} {tot_cd95l_at_percentile(train_proxy[..., 0], train_proxy[..., 1], p):.2e}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"constant_proxy {p} {tot_cd95l_at_percentile(constant_proxy[..., 0], constant_proxy[..., 1], p):.2e}\"\n",
    "    )\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{10**4.3750000e+00 * conversion_factor * 180:.2e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_key(\n",
    "    x: List[dict], sort_key: str, value_keys: str, key_map, value_map\n",
    ") -> Tuple[list, list]:\n",
    "    values = [\n",
    "        (\n",
    "            key_map(v[sort_key]),\n",
    "            value_map(*[v[k] for k in value_keys])\n",
    "            if isinstance(value_keys, (tuple, list))\n",
    "            else value_map(v[value_keys]),\n",
    "        )\n",
    "        for v in x\n",
    "    ]\n",
    "    return sorted(values, key=lambda v: v[0])\n",
    "\n",
    "\n",
    "def final_dead_cells(solution, thresholds):\n",
    "    dead_cells = true_reward_fn((solution, thresholds), instantaneous=True)\n",
    "    return dead_cells[-1]\n",
    "\n",
    "\n",
    "def plot_scan_reward_comparison(\n",
    "    train_extra_infos, constant_extra_infos, save_prefix: str = None\n",
    "):\n",
    "    def plot(style_name):\n",
    "        fig, ax = plt.subplots(\n",
    "            2, 1, sharex=True, figsize=(plot_half_width, plot_half_width)\n",
    "        )\n",
    "\n",
    "        train_proxy = np.asarray(\n",
    "            sort_by_key(\n",
    "                train_extra_infos,\n",
    "                \"mean_conc\",\n",
    "                \"eval_proxy_reward\",\n",
    "                key_map=lambda k: k[0],\n",
    "                value_map=lambda v: v,\n",
    "            )\n",
    "        )\n",
    "        constant_proxy = np.asarray(\n",
    "            sort_by_key(\n",
    "                constant_extra_infos,\n",
    "                \"mean_conc\",\n",
    "                \"eval_proxy_reward\",\n",
    "                key_map=lambda k: k[0],\n",
    "                value_map=lambda v: v,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        train_dead = np.asarray(\n",
    "            sort_by_key(\n",
    "                train_extra_infos,\n",
    "                \"mean_conc\",\n",
    "                (\"eval_solution\", \"eval_threshold\"),\n",
    "                key_map=lambda k: k[0],\n",
    "                value_map=final_dead_cells,\n",
    "            )\n",
    "        )\n",
    "        constant_dead = np.asarray(\n",
    "            sort_by_key(\n",
    "                constant_extra_infos,\n",
    "                \"mean_conc\",\n",
    "                (\"eval_solution\", \"eval_threshold\"),\n",
    "                key_map=lambda k: k[0],\n",
    "                value_map=final_dead_cells,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ax[0].set_xscale(\"log\")\n",
    "        ax[0].set_ylabel(\"Proxy Reward\")\n",
    "        ax[0].plot(\n",
    "            train_proxy[..., 0] * conversion_factor * 180,\n",
    "            train_proxy[..., 1] / 500,\n",
    "            label=\"Optimized Control\",\n",
    "        )\n",
    "        ax[0].plot(\n",
    "            constant_proxy[..., 0] * conversion_factor * 180,\n",
    "            constant_proxy[..., 1] / 500,\n",
    "            label=\"Constant Control\",\n",
    "        )\n",
    "        ax[0].legend()\n",
    "\n",
    "        ax[1].set_xlabel(\"Total CD95L [min*ng/ml]\")\n",
    "        ax[1].set_ylabel(\"Frac. Dead Cells\")\n",
    "        ax[1].plot(\n",
    "            train_dead[..., 0] * conversion_factor * 180,\n",
    "            train_dead[..., 1] / 500,\n",
    "            label=\"Optimized Control\",\n",
    "        )\n",
    "        ax[1].plot(\n",
    "            constant_dead[..., 0] * conversion_factor * 180,\n",
    "            constant_dead[..., 1] / 500,\n",
    "            label=\"Constant Control\",\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        plt.savefig(\n",
    "            result_base_dir + \"/scans/scan_reward_comparison.png\", bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.savefig(\n",
    "            result_base_dir + \"/scans/scan_reward_comparison.svg\", bbox_inches=\"tight\"\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # ax[1].set_xlim([1e3, 4.5e3])\n",
    "        # ax[1].set_ylim([0.4, 0.6])\n",
    "        # ax[0].set_ylim([train_proxy[..., 1].max() / 2.1 / 500, train_proxy[..., 1].max() / 1.9 / 500])\n",
    "        show(\"/scan_reward_comparison\" + style_name, save_prefix=save_prefix)\n",
    "\n",
    "    styles(plot)\n",
    "\n",
    "\n",
    "plot_scan_reward_comparison(\n",
    "    cont_train_extra_infos, constant_extra_infos, save_prefix=\"/scans\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scan_curvegrid(extra_infos, prefix: str, with_norm: bool, use_imshow: bool):\n",
    "    def plot(style_name):\n",
    "        sorted_curves = sort_by_key(\n",
    "            extra_infos,\n",
    "            \"mean_conc\",\n",
    "            \"eval_control\",\n",
    "            key_map=lambda k: k[0],\n",
    "            value_map=lambda v: v.curve.nodes,\n",
    "        )\n",
    "\n",
    "        mean_concs = np.asarray([v[0] for v in sorted_curves])\n",
    "        curves = np.asarray([v[1] for v in sorted_curves])\n",
    "        times = extra_infos[0][\"eval_control\"].curve.times\n",
    "\n",
    "        curves = curves[:, :-1, 0]\n",
    "        if with_norm:\n",
    "            curves = curves / np.sum(curves, axis=1, keepdims=True)\n",
    "\n",
    "        times = times[:-1]\n",
    "\n",
    "        plt.figure(figsize=(plot_half_width, plot_half_width))\n",
    "        plt.xlabel(\"Time [min]\")\n",
    "        plt.ylabel(\"Total CD95L [min*ng/ml]\")\n",
    "\n",
    "        if use_imshow:\n",
    "            from scipy.interpolate import interpn\n",
    "\n",
    "            eval_x = times\n",
    "            eval_y = np.linspace(\n",
    "                np.log(mean_concs[0]), np.log(mean_concs[-1]), len(times)\n",
    "            )\n",
    "            values = interpn(\n",
    "                (times, np.log(mean_concs)),\n",
    "                curves.T,\n",
    "                np.stack(np.meshgrid(eval_x, eval_y), axis=-1),\n",
    "            )\n",
    "\n",
    "            plt.imshow(\n",
    "                values,\n",
    "                cmap=\"magma\",\n",
    "                norm=\"log\",\n",
    "                aspect=\"auto\",\n",
    "                origin=\"lower\",\n",
    "                extent=[\n",
    "                    times[0],\n",
    "                    times[-1],\n",
    "                    np.log10(mean_concs[0] * conversion_factor * 180),\n",
    "                    np.log10(mean_concs[-1] * conversion_factor * 180),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            ax = plt.gca()\n",
    "            ax.yaxis.set_major_formatter(\n",
    "                plt.FuncFormatter(lambda x, pos: f\"$10^{int(x)}$\")\n",
    "            )\n",
    "        else:\n",
    "            plt.yscale(\"log\")\n",
    "            plt.pcolormesh(\n",
    "                times,\n",
    "                mean_concs * conversion_factor * 180,\n",
    "                curves,\n",
    "                shading=\"nearest\",\n",
    "                cmap=\"magma\",\n",
    "                norm=\"log\",\n",
    "            )\n",
    "\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label(\n",
    "            \"Norm. Inst. CD95L [ng/ml]\" if with_norm else \"Inst. CD95L [ng/ml]\"\n",
    "        )\n",
    "\n",
    "        # plt.savefig(result_base_dir + prefix + \"curve_grid.png\", bbox_inches=\"tight\")\n",
    "        # plt.savefig(result_base_dir + prefix + \"curve_grid.svg\", bbox_inches=\"tight\")\n",
    "        # plt.show()\n",
    "        show(\"curve_grid\" + style_name, save_prefix=prefix)\n",
    "\n",
    "    styles(plot)\n",
    "\n",
    "\n",
    "plot_scan_curvegrid(cont_train_extra_infos, \"/scans/continuous/\", False, False)\n",
    "plot_scan_curvegrid(cont_train_extra_infos, \"/scans/continuous/imshow_\", False, True)\n",
    "plot_scan_curvegrid(cont_train_extra_infos, \"/scans/continuous/norm_\", True, False)\n",
    "plot_scan_curvegrid(\n",
    "    cont_train_extra_infos, \"/scans/continuous/imshow_norm_\", True, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_individual_trajectory(extra_infos, target_mean_conc, prefix: str):\n",
    "    def plot(style_name):\n",
    "        # Find closest key to target conc\n",
    "        mean_concs = np.asarray([v[\"mean_conc\"] for v in extra_infos])[:, 0]\n",
    "        closest_idx = np.argmin(np.abs(mean_concs - target_mean_conc))\n",
    "\n",
    "        closest_info = extra_infos[closest_idx]\n",
    "        mean_conc = closest_info[\"mean_conc\"]\n",
    "\n",
    "        constant_control = controls.InterpolationCurveControl(\n",
    "            nn.InterpolationCurve(\n",
    "                method=\"step\", t_start=0.0, t_end=180.0, steps=1, channels=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        (constant_solution, constant_threshold), _ = evaluate_with_integral(\n",
    "            constant_control,\n",
    "            mean_conc,\n",
    "            eval_environment,\n",
    "            eval_environment_state,\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(\n",
    "            3, 1, sharex=True, figsize=(plot_half_width, plot_half_width * 1.5)\n",
    "        )\n",
    "\n",
    "        time = constant_solution.ts\n",
    "\n",
    "        opt_tbid_frac = tbid_fraction(closest_info[\"eval_solution\"].ys)\n",
    "        opt_tbid_frac_mean = jnp.mean(opt_tbid_frac, axis=-1)\n",
    "        opt_tbid_frac_std = jnp.std(opt_tbid_frac, axis=-1)\n",
    "\n",
    "        const_tbid_frac = tbid_fraction(constant_solution.ys)\n",
    "        const_tbid_frac_mean = np.mean(const_tbid_frac, axis=-1)\n",
    "        const_tbid_frac_std = jnp.std(const_tbid_frac, axis=-1)\n",
    "\n",
    "        opt_dead_cells = true_reward_fn(\n",
    "            (closest_info[\"eval_solution\"], closest_info[\"eval_threshold\"]),\n",
    "            instantaneous=True,\n",
    "        )\n",
    "        const_dead_cells = true_reward_fn(\n",
    "            (constant_solution, constant_threshold), instantaneous=True\n",
    "        )\n",
    "\n",
    "        # tBID\n",
    "        ax[0].set_ylabel(\"Rel. tBID\")\n",
    "\n",
    "        ax[0].set_ylim([-0.05, 1.05])\n",
    "        ax[0].fill_between(\n",
    "            time,\n",
    "            const_tbid_frac_mean - const_tbid_frac_std,\n",
    "            const_tbid_frac_mean + const_tbid_frac_std,\n",
    "            color=\"tab:orange\",\n",
    "            alpha=0.25,\n",
    "        )\n",
    "        ax[0].plot(time, const_tbid_frac_mean, c=\"tab:orange\")\n",
    "\n",
    "        ax[0].fill_between(\n",
    "            time,\n",
    "            opt_tbid_frac_mean - opt_tbid_frac_std,\n",
    "            opt_tbid_frac_mean + opt_tbid_frac_std,\n",
    "            color=\"tab:blue\",\n",
    "            alpha=0.25,\n",
    "        )\n",
    "        ax[0].plot(time, opt_tbid_frac_mean, c=\"tab:blue\")\n",
    "\n",
    "        # Number of Cells\n",
    "        ax[1].set_ylabel(\"Frac. Dead\")\n",
    "        ax[1].set_ylim([-0.05, 1.05])\n",
    "        ax[1].plot(time, const_dead_cells / 500, c=\"tab:orange\")\n",
    "        ax[1].plot(time, opt_dead_cells / 500, c=\"tab:blue\")\n",
    "\n",
    "        # Control\n",
    "        ax[2].set_xlabel(\"Time [min]\")\n",
    "        ax[2].set_ylabel(\"CD95L [ng/ml]\")\n",
    "        ax[2].set_yscale(\"log\")\n",
    "\n",
    "        optimal_receptor_activation_inst_cd95l = 2795.5549439517386\n",
    "        ax[2].axhline(optimal_receptor_activation_inst_cd95l, c=\"tab:red\", linestyle=\"--\")\n",
    "\n",
    "        ax[2].plot(\n",
    "            [time[0], time[-1]],\n",
    "            [mean_conc * conversion_factor] * 2,\n",
    "            c=\"tab:orange\",\n",
    "        )\n",
    "\n",
    "        ax[2].plot(\n",
    "            time,\n",
    "            closest_info[\"eval_control\"].curve.nodes * conversion_factor,\n",
    "            c=\"tab:blue\",\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        ax[2].text(\n",
    "            0.95,\n",
    "            0.95,\n",
    "            f\"Total CD95L = {mean_conc.item() * conversion_factor * 180:.1e} min*ng/ml\",\n",
    "            horizontalalignment=\"right\",\n",
    "            verticalalignment=\"top\",\n",
    "            transform=ax[2].transAxes,\n",
    "            size=10,\n",
    "            bbox=dict(boxstyle=\"square\", facecolor=\"white\", alpha=0.75),\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        ax[2].plot([], [], color=\"tab:blue\", label=\"Optimized\")\n",
    "        ax[2].plot([], [], color=\"tab:orange\", label=\"Constant\")\n",
    "        ax[2].plot([], [], linestyle=\"--\", color=\"tab:red\", label=\"Max. Recept. Act.\")\n",
    "        ax[2].legend()\n",
    "\n",
    "        #plt.savefig(\n",
    "        #    result_base_dir + prefix + \"individual_trajectory.png\", bbox_inches=\"tight\"\n",
    "        #)\n",
    "        #plt.savefig(\n",
    "        #    result_base_dir + prefix + \"individual_trajectory.svg\", bbox_inches=\"tight\"\n",
    "        #)\n",
    "        #plt.show()\n",
    "\n",
    "        show(\"individual_trajectory\"+style_name, prefix)\n",
    "\n",
    "    styles(plot)\n",
    "\n",
    "\n",
    "for c in range(-5, 6):\n",
    "    plot_individual_trajectory(cont_train_extra_infos, 10.0**c, f\"/scans/continuous/individual_trajectories/{c}_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CD95 response curve\n",
    "\n",
    "\n",
    "def CD95act(CD95L, x):\n",
    "    k = [None] * 11\n",
    "\n",
    "    k[0] = 8.12e-4  # kon,FADD\n",
    "    k[1] = 0.00567  # koff,FADD\n",
    "    k[2] = 0.000492  # kon,p55\n",
    "    k[3] = 0.0114  # kcl,D216\n",
    "    k[4] = 4.47e-4  # kcl,D374,trans,p55\n",
    "    k[5] = 0.00344  # kcl,D374,trans,p43\n",
    "    k[6] = 0.0950  # kp18,inactive\n",
    "    k[7] = 0.000529  # kcl,BID\n",
    "    k[8] = 0.00152  # kcl,probe\n",
    "    k[9] = 8.98  # KD,R\n",
    "    k[10] = 15.4  # KD,L\n",
    "\n",
    "    # Active CD95 receptors, steady state solution (in response to CD95L / control)\n",
    "    CD95act = (\n",
    "        x[0] ** 3\n",
    "        * k[10] ** 2\n",
    "        * CD95L\n",
    "        / (\n",
    "            (CD95L + k[10])\n",
    "            * (\n",
    "                x[0] ** 2 * k[10] ** 2\n",
    "                + k[9] * CD95L**2\n",
    "                + 2 * k[9] * k[10] * CD95L\n",
    "                + k[9] * k[10] ** 2\n",
    "            )\n",
    "        )\n",
    "    )  # CD95act\n",
    "\n",
    "    return CD95act\n",
    "\n",
    "\n",
    "ligand_moles = 10 ** np.linspace(-1, 4, 1024)\n",
    "ligand_concentration = ligand_moles * conversion_factor\n",
    "\n",
    "act = 0\n",
    "for r in eval_environment_state.x0[:, 0]:\n",
    "    receptor = r\n",
    "    act = act + CD95act(ligand_moles, [receptor]) / receptor\n",
    "\n",
    "act = act / eval_environment_state.x0.shape[0]\n",
    "\n",
    "print(\n",
    "    ligand_concentration[np.argmax(act)], ligand_concentration[np.argmax(act)] * 180.0\n",
    ")\n",
    "\n",
    "\n",
    "def plot(style_name):\n",
    "    plt.figure(figsize=(plot_half_width, plot_third_width))\n",
    "    plt.xlabel(\"CD95L [ng/ml]\")\n",
    "    plt.ylabel(\"Avg. Norm. Act.\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.plot(ligand_concentration, act)\n",
    "    # plt.savefig(result_base_dir + \"/receptor_activation.png\", bbox_inches=\"tight\")\n",
    "    # plt.savefig(result_base_dir + \"/receptor_activation.svg\", bbox_inches=\"tight\")\n",
    "    # plt.show()\n",
    "    show(\"/receptor_activation\" + style_name, save_prefix=\"\")\n",
    "\n",
    "\n",
    "styles(plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39-optimal-control-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
