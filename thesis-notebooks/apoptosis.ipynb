{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import evosax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from jaxtyping import Array, ArrayLike, Scalar\n",
    "\n",
    "import optimal_control.constraints as constraints\n",
    "import optimal_control.controls as controls\n",
    "import optimal_control.environments.examples as examples\n",
    "import optimal_control.nn as nn\n",
    "import optimal_control.solvers as solvers\n",
    "import optimal_control.trainers as trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random key for reproducibility\n",
    "key = jax.random.PRNGKey(1234)\n",
    "\n",
    "# Initialize environment\n",
    "environment: examples.ApoptosisEnvironment = examples.ApoptosisEnvironment(\n",
    "    \"../data/Initial_concentrations_CD95H_wtH.mat\", [0, 500], 50\n",
    ")\n",
    "environment_state = environment.init()\n",
    "\n",
    "# Build controller\n",
    "key, subkey = jax.random.split(key)\n",
    "control = controls.ImplicitTemporalControl(\n",
    "    implicit_fn=nn.Siren(\n",
    "        in_features=1, out_features=2, hidden_features=64, hidden_layers=2, key=subkey\n",
    "    ),\n",
    "    t_start=0.0,\n",
    "    t_end=180.0,\n",
    "    to_curve=True,\n",
    "    curve_interpolation=\"linear\",\n",
    "    curve_steps=181,\n",
    ")\n",
    "\n",
    "\n",
    "# Define reward function\n",
    "def reward_fn(args: Tuple[diffrax.Solution, Array]):\n",
    "    solution, thresh = args\n",
    "    ys = solution.ys\n",
    "\n",
    "    reward = jnp.mean(\n",
    "        jnp.clip(\n",
    "            ys[..., 12] / (ys[..., 3] + ys[..., 12]),\n",
    "            a_min=None,\n",
    "            a_max=thresh.reshape(-1, 1),\n",
    "        )\n",
    "    )\n",
    "    return reward\n",
    "\n",
    "\n",
    "# Make direct and ES solver\n",
    "direct_solver = solvers.DirectSolver(optax.adam(learning_rate=3e-4))\n",
    "\n",
    "evo_control_params = eqx.filter(control, eqx.is_array)\n",
    "evo_parameter_reshaper = evosax.ParameterReshaper(evo_control_params)\n",
    "evo_fitness_shaper = evosax.FitnessShaper(centered_rank=True, maximize=True)\n",
    "\n",
    "evo_strategy = evosax.OpenES(\n",
    "    popsize=64,\n",
    "    num_dims=len(evo_parameter_reshaper.flatten_single(evo_control_params)),\n",
    ")\n",
    "evo_strategy_params = evo_strategy.params_strategy\n",
    "\n",
    "evo_solver = solvers.ESSolver(\n",
    "    evo_strategy, evo_strategy_params, evo_parameter_reshaper, evo_fitness_shaper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helper\n",
    "@eqx.filter_jit\n",
    "def train_with_integral(\n",
    "    control: controls.AbstractControl,\n",
    "    solver: solvers.AbstractSolver,\n",
    "    target_integral: Array,\n",
    ") -> Tuple[Scalar, controls.AbstractControl]:\n",
    "    constraint_chain = constraints.ConstraintChain(\n",
    "        transformations=[\n",
    "            constraints.NonNegativeConstantIntegralConstraint(target=target_integral)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    opt_reward, opt_control = trainers.solve_optimal_control_problem(\n",
    "        num_train_steps=1024,\n",
    "        environment=environment,\n",
    "        reward_fn=reward_fn,\n",
    "        constraint_chain=constraint_chain,\n",
    "        solver=solver,\n",
    "        control=control,\n",
    "        key=key,\n",
    "        pbar_interval=8,\n",
    "    )\n",
    "\n",
    "    return opt_reward, opt_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run train and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_reward, direct_control = train_with_integral(\n",
    "    control, direct_solver, jnp.asarray([1.0])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39-optimal-control-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
