{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "import time\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import evosax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from adaptive import Learner2D\n",
    "from adaptive.notebook_integration import notebook_extension\n",
    "from jaxtyping import Array, ArrayLike, PRNGKeyArray, PyTree, Scalar\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import optimal_control.constraints as constraints\n",
    "import optimal_control.controls as controls\n",
    "import optimal_control.environments.examples as examples\n",
    "import optimal_control.nn as nn\n",
    "import optimal_control.solvers as solvers\n",
    "import optimal_control.trainers as trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_inches = (8.3, 11.7)\n",
    "plot_full_width = a4_inches[0]\n",
    "plot_half_width = a4_inches[0] / 2\n",
    "plot_third_width = a4_inches[0] / 3\n",
    "plot_quarter_width = a4_inches[0] / 4\n",
    "\n",
    "result_base_dir = \"../thesis-results/apoptosis\"\n",
    "plot_style = \"seaborn-paper\"\n",
    "\n",
    "plot_shrink_factor = 0.9\n",
    "\n",
    "plt.style.use(plot_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random key for reproducibility\n",
    "key = jax.random.PRNGKey(1234)\n",
    "\n",
    "# Initialize environment\n",
    "environment: examples.ApoptosisEnvironment = examples.ApoptosisEnvironment(\n",
    "    \"../data/Initial_concentrations_CD95H_wtH.mat\", [0, 500], 50\n",
    ")\n",
    "environment_state = environment.init()\n",
    "\n",
    "eval_environment: examples.ApoptosisEnvironment = examples.ApoptosisEnvironment(\n",
    "    \"../data/Initial_concentrations_CD95H_wtH.mat\", [500, 1000], -1, True\n",
    ")\n",
    "eval_environment_state = eval_environment.init()\n",
    "\n",
    "# Build controller\n",
    "key, subkey = jax.random.split(key)\n",
    "control = controls.ImplicitTemporalControl(\n",
    "    implicit_fn=nn.Siren(\n",
    "        in_features=1, out_features=1, hidden_features=64, hidden_layers=2, key=subkey\n",
    "    ),\n",
    "    t_start=0.0,\n",
    "    t_end=180.0,\n",
    "    to_curve=True,\n",
    "    curve_interpolation=\"linear\",\n",
    "    curve_steps=181,\n",
    ")\n",
    "\n",
    "\n",
    "# Define reward function\n",
    "def proxy_reward_fn(\n",
    "    args: Tuple[diffrax.Solution, Array], instantaneous: bool = False, norm: bool = True\n",
    "):\n",
    "    # Continuous fraction of tBID, clipped at the tBID-apoptosis threshold\n",
    "\n",
    "    solution, thresh = args\n",
    "    ys = solution.ys\n",
    "\n",
    "    tBID_frac = ys[..., 12] / (ys[..., 3] + ys[..., 12])\n",
    "    if norm:\n",
    "        frac_norm = tBID_frac / thresh\n",
    "        frac_clipped = jnp.clip(frac_norm, a_max=1.0)\n",
    "    else:\n",
    "        frac_clipped = jnp.clip(tBID_frac, a_max=thresh)\n",
    "\n",
    "    if instantaneous:\n",
    "        reward = jnp.sum(frac_clipped, axis=-1)\n",
    "    else:\n",
    "        reward = jnp.mean(jnp.sum(frac_clipped, axis=-1))\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def true_reward_fn(args: Tuple[diffrax.Solution, Array], instantaneous: bool = False):\n",
    "    # Number of dead cells\n",
    "\n",
    "    solution, thresh = args\n",
    "    ys = solution.ys\n",
    "\n",
    "    tBID_frac = ys[..., 12] / (ys[..., 3] + ys[..., 12])\n",
    "    tBID_above = tBID_frac > thresh\n",
    "\n",
    "    if instantaneous:\n",
    "        reward = jnp.sum(tBID_above, axis=-1)\n",
    "    else:\n",
    "        reward = jnp.mean(jnp.sum(tBID_above, axis=-1))\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "# Direct solver\n",
    "direct_solver = solvers.DirectSolver(optax.adam(learning_rate=3e-4))\n",
    "\n",
    "\n",
    "# ES Solver factory (to allow for init parameters)\n",
    "def make_es_solver(init_control: controls.AbstractControl) -> solvers.ESSolver:\n",
    "    evo_control_params = eqx.filter(init_control, eqx.is_array)\n",
    "    evo_parameter_reshaper = evosax.ParameterReshaper(evo_control_params)\n",
    "    evo_fitness_shaper = evosax.FitnessShaper(centered_rank=True, maximize=True)\n",
    "\n",
    "    # \"\"\"\n",
    "    evo_strategy = evosax.OpenES(\n",
    "        popsize=64,\n",
    "        lrate_init=3e-4,\n",
    "        lrate_limit=3e-4,\n",
    "        sigma_init=1e-3,\n",
    "        sigma_limit=1e-3,\n",
    "        num_dims=len(evo_parameter_reshaper.flatten_single(evo_control_params)),\n",
    "    )\n",
    "    # \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    evo_strategy = evosax.LES(\n",
    "        popsize=64,\n",
    "        num_dims=len(evo_parameter_reshaper.flatten_single(evo_control_params)),\n",
    "        net_ckpt_path=\"../data/2023_03_les_v1.pkl\",\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    evo_strategy_params = evo_strategy.default_params\n",
    "\n",
    "    evo_solver: solvers.ESSolver = solvers.ESSolver(\n",
    "        evo_strategy, evo_strategy_params, evo_parameter_reshaper, evo_fitness_shaper\n",
    "    )\n",
    "\n",
    "    return evo_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helper\n",
    "@eqx.filter_jit\n",
    "def train_with_integral(\n",
    "    control: controls.AbstractControl,\n",
    "    solver: solvers.AbstractSolver,\n",
    "    target_integral: Array,\n",
    "    reward_fn=proxy_reward_fn,\n",
    "    num_steps: int = 1024,\n",
    ") -> Tuple[Scalar, controls.AbstractControl]:\n",
    "    constraint_chain = constraints.ConstraintChain(\n",
    "        transformations=[\n",
    "            constraints.NonNegativeConstantIntegralConstraint(target=target_integral)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    opt_reward, opt_control = trainers.solve_optimal_control_problem(\n",
    "        num_train_steps=num_steps,\n",
    "        environment=environment,\n",
    "        reward_fn=reward_fn,\n",
    "        constraint_chain=constraint_chain,\n",
    "        solver=solver,\n",
    "        control=control,\n",
    "        key=key,\n",
    "        pbar_interval=8,\n",
    "        integrate_kwargs=dict(vmap=\"inner\"),\n",
    "    )\n",
    "\n",
    "    return opt_reward, opt_control\n",
    "\n",
    "@eqx.filter_jit\n",
    "def evaluate_with_integral(\n",
    "    control: controls.AbstractControl,\n",
    "    target_integral: Array,\n",
    "    environment: examples.ApoptosisEnvironment,\n",
    "    state: examples.ApoptosisState,\n",
    ") -> Tuple[PyTree, controls.AbstractControl]:\n",
    "    constraint_chain = constraints.ConstraintChain(\n",
    "        transformations=[\n",
    "            constraints.NonNegativeConstantIntegralConstraint(target=target_integral)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    constrained_control, _ = solvers.build_control(control, constraint_chain)\n",
    "\n",
    "    solution = environment.integrate(constrained_control, state, key, vmap=\"inner\")\n",
    "    return solution, constrained_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between grad, es, and rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_reward, pretrained_control = train_with_integral(\n",
    "    control,\n",
    "    direct_solver,\n",
    "    jnp.asarray([1.0]),\n",
    "    reward_fn=proxy_reward_fn,\n",
    "    num_steps=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_reward, finetuned_control = train_with_integral(\n",
    "    pretrained_control,\n",
    "    make_es_solver(pretrained_control),\n",
    "    jnp.asarray([1.0]),\n",
    "    reward_fn=true_reward_fn,\n",
    "    num_steps=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_es_reward, only_es_control = train_with_integral(\n",
    "    control,\n",
    "    make_es_solver(pretrained_control),\n",
    "    jnp.asarray([1.0]),\n",
    "    reward_fn=true_reward_fn,\n",
    "    num_steps=1024 + 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pretrain_solution, tBID_thresh), pretrain_constrained_control = evaluate_with_integral(\n",
    "    pretrained_control, jnp.asarray([1.0]), eval_environment, eval_environment_state\n",
    ")\n",
    "\n",
    "(\n",
    "    finetuned_solution,\n",
    "    tBID_thresh,\n",
    "), finetune_constrained_control = evaluate_with_integral(\n",
    "    finetuned_control, jnp.asarray([1.0]), eval_environment, eval_environment_state\n",
    ")\n",
    "\n",
    "(\n",
    "    only_es_solution,\n",
    "    tBID_thresh,\n",
    "), only_es_constrained_control = evaluate_with_integral(\n",
    "    only_es_control, jnp.asarray([1.0]), eval_environment, eval_environment_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_proxy_reward = proxy_reward_fn(\n",
    "    (pretrain_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "pretrain_true_reward = true_reward_fn(\n",
    "    (pretrain_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "\n",
    "finetune_proxy_reward = proxy_reward_fn(\n",
    "    (finetuned_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "finetune_true_reward = true_reward_fn(\n",
    "    (finetuned_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "\n",
    "only_es_proxy_reward = proxy_reward_fn(\n",
    "    (only_es_solution, tBID_thresh), instantaneous=True\n",
    ")\n",
    "only_es_true_reward = true_reward_fn(\n",
    "    (only_es_solution, tBID_thresh), instantaneous=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.mean(pretrain_true_reward), jnp.mean(finetune_true_reward))\n",
    "print(pretrain_true_reward[-1], finetune_true_reward[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqx.tree_serialise_leaves(result_base_dir + \"/reward_comparison/pretrain_control.eqx\", pretrained_control)\n",
    "eqx.tree_serialise_leaves(result_base_dir + \"/reward_comparison/finetune_control.eqx\", finetuned_control)\n",
    "eqx.tree_serialise_leaves(result_base_dir + \"/reward_comparison/es_control.eqx\", only_es_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pretrain_solution.ts\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(plot_half_width, plot_half_width))\n",
    "\n",
    "ax[0].set_ylabel(\"True Reward\")\n",
    "ax[0].plot(ts, pretrain_true_reward, label=\"Proxy\")\n",
    "ax[0].plot(ts, finetune_true_reward, linestyle=\"--\", label=\"Proxy -> True\")\n",
    "ax[0].plot(ts, only_es_true_reward, label=\"True\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_xlabel(\"Time [min.]\")\n",
    "ax[1].set_ylabel(\"Proxy Reward\")\n",
    "ax[1].plot(ts, pretrain_proxy_reward, label=\"Proxy\")\n",
    "ax[1].plot(ts, finetune_proxy_reward, linestyle=\"--\", label=\"Proxy -> True\")\n",
    "ax[1].plot(ts, only_es_proxy_reward, label=\"True\")\n",
    "\n",
    "plt.savefig(\n",
    "    result_base_dir + \"/reward_comparison/proxy_and_true_reward.png\",\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.savefig(\n",
    "    result_base_dir + \"/reward_comparison/proxy_and_true_reward.svg\",\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pretrain_solution.ts\n",
    "p_cs = jax.vmap(pretrain_constrained_control)(ts) \n",
    "f_cs = jax.vmap(finetune_constrained_control)(ts) \n",
    "es_cs = jax.vmap(only_es_constrained_control)(ts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(plot_half_width, plot_half_width))\n",
    "\n",
    "plt.xlabel(\"Time [min.]\")\n",
    "plt.ylabel(\"CD95L [ng/ml]\")\n",
    "plt.plot(ts, p_cs, label=\"Proxy\")\n",
    "plt.plot(ts, f_cs, linestyle=\"--\", label=\"Proxy -> True\")\n",
    "plt.plot(ts, es_cs, label=\"True\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\n",
    "    result_base_dir + \"/reward_comparison/proxy_and_true_controls.png\",\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.savefig(\n",
    "    result_base_dir + \"/reward_comparison/proxy_and_true_controls.svg\",\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show loss landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate region around optimum\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def sample_direction(network: PyTree, key: PRNGKeyArray) -> PyTree:\n",
    "    jax_params = eqx.filter(network, eqx.is_array)\n",
    "    reshaper = evosax.ParameterReshaper(eqx.filter(network, eqx.is_array))\n",
    "    flat_params = reshaper.flatten_single(jax_params)\n",
    "    noise = jax.random.normal(key, flat_params.shape, flat_params.dtype)\n",
    "    direction = noise / jnp.sqrt(jnp.sum(jnp.square(noise)))\n",
    "\n",
    "    return direction\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def scan_2d(\n",
    "    control: controls.AbstractControl, x: Array, y: Array, xs: Array, ys: Array\n",
    "):\n",
    "    jax_params, jax_static = eqx.partition(control, eqx.is_array)\n",
    "    reshaper = evosax.ParameterReshaper(jax_params)\n",
    "    source_params = reshaper.flatten_single(jax_params)\n",
    "    offsets = y[None, None] * ys[:, None, None] + x[None, None] * xs[None, :, None]\n",
    "    offsets = offsets.reshape(-1, len(source_params))\n",
    "    grid_params = source_params + offsets\n",
    "\n",
    "    # \"\"\"\n",
    "    grid_controls = eqx.combine(reshaper.reshape(grid_params), jax_static)\n",
    "\n",
    "    (\n",
    "        grid_solutions,\n",
    "        grid_tbid_thresholds,\n",
    "    ), _ = eqx.filter_vmap(\n",
    "        partial(\n",
    "            evaluate_with_integral,\n",
    "            target_integral=jnp.asarray([1.0]),\n",
    "            environment=eval_environment,\n",
    "            state=eval_environment_state,\n",
    "        )\n",
    "    )(grid_controls)\n",
    "    # \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    grid_controls = eqx.combine(reshaper.reshape(grid_params), jax_static)\n",
    "    grid_solutions, grid_tBID_thresholds = jax.lax.map(\n",
    "        partial(\n",
    "            evaluate_with_integral,\n",
    "            target_integral=jnp.asarray([1.0]),\n",
    "            environment=eval_environment,\n",
    "            state=eval_environment_state,\n",
    "        ),\n",
    "        grid_controls,\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    grid_solutions = []\n",
    "    grid_tbid_thresholds = []\n",
    "    for params in grid_params:\n",
    "        grid_control = eqx.combine(reshaper.reshape_single(params), jax_static)\n",
    "        grid_solution, grid_tbid_threshold = evaluate_with_integral(\n",
    "            control=grid_control,\n",
    "            target_integral=jnp.asarray([1.0]),\n",
    "            environment=eval_environment,\n",
    "            state=eval_environment_state,\n",
    "        )\n",
    "\n",
    "        grid_solutions.append(grid_solutions)\n",
    "        grid_tbid_thresholds.append(grid_tbid_thresholds)\n",
    "\n",
    "    grid_solutions = jax.tree_map(\n",
    "        lambda x: jnp.stack(x, axis=0) if eqx.is_array(x[0]) else x[0],\n",
    "        grid_solutions[0],\n",
    "        grid_solutions[1:],\n",
    "    )\n",
    "    grid_tbid_thresholds = jnp.stack(grid_tbid_thresholds, axis=0)\n",
    "    \"\"\"\n",
    "\n",
    "    return grid_solutions, grid_tbid_thresholds\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def sample_control(\n",
    "    control: controls.AbstractControl, x_dir: Array, y_dir: Array, x: Scalar, y: Scalar\n",
    ") -> controls.AbstractControl:\n",
    "    jax_params, jax_static = eqx.partition(control, eqx.is_array)\n",
    "    reshaper = evosax.ParameterReshaper(jax_params)\n",
    "    source_params = reshaper.flatten_single(jax_params)\n",
    "\n",
    "    offset = y_dir * y + x_dir * x\n",
    "    offset_params = source_params + offset\n",
    "    offset_params = reshaper.reshape_single(offset_params)\n",
    "    offset_control = eqx.combine(offset_params, jax_static)\n",
    "\n",
    "    return offset_control\n",
    "\n",
    "\n",
    "def adaptive_2d(\n",
    "    control: controls.AbstractControl,\n",
    "    x_dir: Array,\n",
    "    y_dir: Array,\n",
    "    bounds: Tuple[float],\n",
    "    reward_fn,\n",
    "    max_points: int = 1204,\n",
    "):\n",
    "    learner = Learner2D(lambda _: 0.0, bounds=bounds)\n",
    "    learner.stack_size = 1\n",
    "\n",
    "    last_plt_time = time.time()\n",
    "    plt_interval = 15\n",
    "\n",
    "    while learner.npoints < max_points:\n",
    "        try:\n",
    "            points, _ = learner.ask(1)\n",
    "            point = points[0]\n",
    "            x, y = point\n",
    "\n",
    "            offset_control = sample_control(\n",
    "                control, x_dir, y_dir, jnp.float_(x), jnp.float_(y)\n",
    "            )\n",
    "            offset_solution, _ = evaluate_with_integral(\n",
    "                offset_control,\n",
    "                jnp.asarray([1.0]),\n",
    "                eval_environment,\n",
    "                eval_environment_state,\n",
    "            )\n",
    "            offset_reward = reward_fn(offset_solution)\n",
    "            learner.tell(point, float(offset_reward))\n",
    "\n",
    "            if time.time() - last_plt_time >= plt_interval:\n",
    "                clear_output(wait=True)\n",
    "                display(learner.plot(tri_alpha=0.25))\n",
    "                last_plt_time = time.time()\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xkey, ykey = jax.random.split(key)\n",
    "\n",
    "x = sample_direction(pretrained_control, xkey)\n",
    "y = sample_direction(pretrained_control, ykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This consumes lots of RAM\n",
    "\n",
    "grid_solutions, grid_tBID_thresholds = scan_2d(\n",
    "    pretrained_control, x, y, jnp.linspace(-1.5, 1.5, 16), jnp.linspace(-1.5, 1.5, 16)\n",
    ")\n",
    "\n",
    "es_grid_solutions, es_grid_tBID_thresholds = scan_2d(\n",
    "    only_es_control, x, y, jnp.linspace(-1.5, 1.5, 16), jnp.linspace(-1.5, 1.5, 16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_proxy_learner = adaptive_2d(\n",
    "    pretrained_control,\n",
    "    x,\n",
    "    y,\n",
    "    [(-1.5, 1.5), (-1.5, 1.5)],\n",
    "    reward_fn=proxy_reward_fn,\n",
    "    max_points=1024,\n",
    ")\n",
    "\n",
    "pretrain_true_learner = adaptive_2d(\n",
    "    pretrained_control,\n",
    "    x,\n",
    "    y,\n",
    "    [(-1.5, 1.5), (-1.5, 1.5)],\n",
    "    reward_fn=true_reward_fn,\n",
    "    max_points=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_proxy_learner = adaptive_2d(\n",
    "    only_es_control,\n",
    "    x,\n",
    "    y,\n",
    "    [(-1.5, 1.5), (-1.5, 1.5)],\n",
    "    reward_fn=proxy_reward_fn,\n",
    "    max_points=1024,\n",
    ")\n",
    "\n",
    "es_true_learner = adaptive_2d(\n",
    "    only_es_control,\n",
    "    x,\n",
    "    y,\n",
    "    [(-1.5, 1.5), (-1.5, 1.5)],\n",
    "    reward_fn=true_reward_fn,\n",
    "    max_points=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_proxy_learner.save(result_base_dir + \"/reward_comparison/pretrain_proxy_learner.pickle\")\n",
    "pretrain_true_learner.save(result_base_dir + \"/reward_comparison/pretrain_true_learner.pickle\")\n",
    "es_proxy_learner.save(result_base_dir + \"/reward_comparison/es_proxy_learner.pickle\")\n",
    "es_true_learner.save(result_base_dir + \"/reward_comparison/es_true_learner.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot from vmapped grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_proxy_rewards = eqx.filter_vmap(proxy_reward_fn)((grid_solutions, grid_tBID_thresholds))\n",
    "grid_proxy_no_norm_rewards = eqx.filter_vmap(partial(proxy_reward_fn, norm=False))((grid_solutions, grid_tBID_thresholds))\n",
    "grid_true_rewards = eqx.filter_vmap(true_reward_fn)((grid_solutions, grid_tBID_thresholds))\n",
    "\n",
    "es_grid_proxy_rewards = eqx.filter_vmap(proxy_reward_fn)((grid_solutions, grid_tBID_thresholds))\n",
    "es_grid_proxy_no_norm_rewards = eqx.filter_vmap(partial(proxy_reward_fn, norm=False))((grid_solutions, grid_tBID_thresholds))\n",
    "es_grid_true_rewards = eqx.filter_vmap(true_reward_fn)((grid_solutions, grid_tBID_thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter([0.0], [0.0], c=\"red\", marker=\"x\")\n",
    "plt.imshow(\n",
    "    grid_proxy_rewards.reshape(16, 16), cmap=\"magma\", extent=(-1.5, 1.5, -1.5, 1.5)\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter([0.0], [0.0], c=\"red\", marker=\"x\")\n",
    "plt.imshow(\n",
    "    grid_proxy_no_norm_rewards.reshape(16, 16), cmap=\"magma\", extent=(-1.5, 1.5, -1.5, 1.5)\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter([0.0], [0.0], c=\"red\", marker=\"x\")\n",
    "plt.imshow(\n",
    "    grid_true_rewards.reshape(16, 16), cmap=\"magma\", extent=(-1.5, 1.5, -1.5, 1.5)\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot from adaptive grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_proxy_reward_grid = pretrain_proxy_learner.interpolated_on_grid()\n",
    "pretrain_true_reward_grid = pretrain_true_learner.interpolated_on_grid()\n",
    "es_proxy_reward_grid = es_proxy_learner.interpolated_on_grid()\n",
    "es_true_reward_grid = es_true_learner.interpolated_on_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resize_right import resize, interp_methods\n",
    "\n",
    "\n",
    "def plot_reward_grid(grid_x, grid_y, grid_value, label: str, filepath: str):\n",
    "    plt.figure(\n",
    "        figsize=(\n",
    "            plot_half_width * plot_shrink_factor,\n",
    "            plot_half_width * plot_shrink_factor,\n",
    "        )\n",
    "    )\n",
    "    plt.xlabel(\"X Offset [a.u.]\")\n",
    "    plt.ylabel(\"Y Offset [a.u.]\")\n",
    "    plt.imshow(\n",
    "        grid_value, cmap=\"magma\", extent=(grid_x[0], grid_x[-1], grid_y[0], grid_y[-1])\n",
    "    )\n",
    "    cbar = plt.colorbar(fraction=0.04575, pad=0.04)\n",
    "    cbar.set_label(label)\n",
    "    plt.savefig(result_base_dir + filepath + \".png\", bbox_inches=\"tight\")\n",
    "    plt.savefig(result_base_dir + filepath + \".svg\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_diff_grid(grid_x, grid_y, grid1_value, grid2_value, filepath: str):\n",
    "    grid1_value = resize(\n",
    "        grid1_value,\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "        pad_mode=\"edge\",\n",
    "    )\n",
    "    grid2_value = resize(\n",
    "        grid2_value,\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "        pad_mode=\"edge\",\n",
    "    )\n",
    "\n",
    "    # \"\"\"\n",
    "    grid1_value = (grid1_value - grid1_value.min()) / (\n",
    "        grid1_value.max() - grid1_value.min()\n",
    "    )\n",
    "    grid2_value = (grid2_value - grid2_value.min()) / (\n",
    "        grid2_value.max() - grid2_value.min()\n",
    "    )\n",
    "    # \"\"\"\n",
    "\n",
    "    # While znorm leads to easy-to-interpret differences, the data isn't normally\n",
    "    # distributed\n",
    "    \"\"\"\n",
    "    grid1_value = (grid1_value - grid1_value.mean()) / grid1_value.std()\n",
    "    grid2_value = (grid2_value - grid2_value.mean()) / grid2_value.std()\n",
    "    \"\"\"\n",
    "\n",
    "    # grid1_value = grid1_value / grid1_value.mean()\n",
    "    # grid2_value = grid2_value / grid2_value.mean()\n",
    "\n",
    "    grid_diff = grid1_value - grid2_value\n",
    "    vabs = np.abs(grid_diff).max()\n",
    "\n",
    "    plt.figure(\n",
    "        figsize=(\n",
    "            plot_half_width * plot_shrink_factor,\n",
    "            plot_half_width * plot_shrink_factor,\n",
    "        )\n",
    "    )\n",
    "    plt.xlabel(\"X Offset [a.u.]\")\n",
    "    plt.ylabel(\"Y Offset [a.u.]\")\n",
    "    plt.imshow(\n",
    "        grid_diff,\n",
    "        cmap=\"RdBu\",\n",
    "        extent=(grid_x[0], grid_x[-1], grid_y[0], grid_y[-1]),\n",
    "        vmin=-vabs,\n",
    "        vmax=vabs,\n",
    "    )\n",
    "    cbar = plt.colorbar(fraction=0.04575, pad=0.04)\n",
    "    cbar.set_label(\"Diff. between norm. rewards\")\n",
    "    plt.savefig(result_base_dir + filepath + \".png\", bbox_inches=\"tight\")\n",
    "    plt.savefig(result_base_dir + filepath + \".svg\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "grid_x, grid_y = pretrain_proxy_reward_grid[:2]\n",
    "\n",
    "plot_reward_grid(\n",
    "    *pretrain_proxy_reward_grid,\n",
    "    label=\"Proxy Reward\",\n",
    "    filepath=\"/reward_comparison/pretrain_proxy_reward_grid\"\n",
    ")\n",
    "plot_reward_grid(\n",
    "    *pretrain_true_reward_grid,\n",
    "    label=\"True Reward\",\n",
    "    filepath=\"/reward_comparison/pretrain_true_reward_grid\"\n",
    ")\n",
    "\n",
    "plot_reward_grid(\n",
    "    *es_proxy_reward_grid,\n",
    "    label=\"Proxy Reward\",\n",
    "    filepath=\"/reward_comparison/es_proxy_reward_grid\"\n",
    ")\n",
    "plot_reward_grid(\n",
    "    *es_true_reward_grid,\n",
    "    label=\"True Reward\",\n",
    "    filepath=\"/reward_comparison/es_true_reward_grid\"\n",
    ")\n",
    "\n",
    "plot_diff_grid(\n",
    "    grid_x,\n",
    "    grid_y,\n",
    "    pretrain_proxy_reward_grid[2],\n",
    "    pretrain_true_reward_grid[2],\n",
    "    filepath=\"/reward_comparison/pretrain_diff_reward_grid\",\n",
    ")\n",
    "\n",
    "plot_diff_grid(\n",
    "    grid_x,\n",
    "    grid_y,\n",
    "    es_proxy_reward_grid[2],\n",
    "    es_true_reward_grid[2],\n",
    "    filepath=\"/reward_comparison/es_diff_reward_grid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(\n",
    "    resize(\n",
    "        pretrain_proxy_reward_grid[2],\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "    ).flatten(),\n",
    "    bins=128,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    resize(\n",
    "        pretrain_proxy_reward_grid[2],\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "    ).flatten(),\n",
    "    resize(\n",
    "        pretrain_true_reward_grid[2],\n",
    "        out_shape=(64, 64),\n",
    "        interp_method=interp_methods.linear,\n",
    "    ).flatten(),\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39-optimal-control-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
